{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Generic\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# Images\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "import talos as ta\n",
    "import scikitplot as skplt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, plot_confusion_matrix, roc_curve\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Keras\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.utils import print_summary\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.applications.densenet import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from dcgan import DCGAN\n",
    "from dcgan_ext import DCGAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = '/home/ygala/TFM_UOC/'\n",
    "path_images = os.path.join(path_root, 'data', 'covid_images')\n",
    "path_dcgan = os.path.join(path_images, 'dcgan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_split = [0.7, 0.3, 0.0]\n",
    "input_shape = (224, 224, 3)\n",
    "seed = 1234\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "### Hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 1000\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chexNet weights\n",
    "\n",
    "chexnet_weights = '/home/ygala/TFM_UOC/scripts/chexnet/best_weights.h5'\n",
    "\n",
    "def chexnet_preprocess_input(value):\n",
    "    return preprocess_input(value)\n",
    "\n",
    "\n",
    "def get_chexnet_model():\n",
    "    input_shape = (224, 224, 3)\n",
    "    img_input = Input(shape=input_shape)\n",
    "    base_weights = 'imagenet'\n",
    "\n",
    "    # create the base pre-trained model\n",
    "    base_model = DenseNet121(\n",
    "        include_top=False,\n",
    "        input_tensor=img_input,\n",
    "        input_shape=input_shape,\n",
    "        weights=base_weights,\n",
    "        pooling='avg'\n",
    "    )\n",
    "\n",
    "    x = base_model.output\n",
    "    # add a logistic layer -- let's say we have 14 classes\n",
    "    predictions = Dense(\n",
    "        14,\n",
    "        activation='sigmoid',\n",
    "        name='predictions')(x)\n",
    "\n",
    "    # this is the model we will use\n",
    "    model = Model(\n",
    "        inputs=img_input,\n",
    "        outputs=predictions,\n",
    "    )\n",
    "\n",
    "    # load chexnet weights\n",
    "    model.load_weights(chexnet_weights)\n",
    "\n",
    "    # return model\n",
    "    return base_model, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### funciones\n",
    "\n",
    "def get_class_weight(csv_file_path, target_class, learning_rate):\n",
    "    df = pd.read_csv(csv_file_path, sep=';')\n",
    "    total_counts = df.shape[0]\n",
    "    class_weight = []\n",
    "\n",
    "    ratio_pos = df.loc[(df[target_class] == 'Y')].shape[0] / total_counts\n",
    "    ratio_neg = df.loc[(df[target_class] == 'N')].shape[0] / total_counts\n",
    "    class_weight = np.array((ratio_pos, ratio_neg))\n",
    "        \n",
    "    return class_weight\n",
    "\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    # any tensorflow metric\n",
    "    value, update_op = tf.metrics.auc(y_true, y_pred)\n",
    "\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc' in i.name.split('/')[1]]\n",
    "\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "    return value\n",
    "\n",
    "def get_model(learning_rate):\n",
    "    # get base model, model\n",
    "    base_model, chexnet_model = get_chexnet_model()\n",
    "    # print a model summary\n",
    "    # print_summary(base_model)\n",
    "\n",
    "    x = base_model.output\n",
    "    # Dropout layer\n",
    "    #x = Dropout(0.2)(x)\n",
    "    # one more layer (relu)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    # Dropout layer\n",
    "    #x = Dropout(0.2)(x)\n",
    "    #x = Dense(256, activation='relu')(x)\n",
    "    # Dropout layer\n",
    "    #x = Dropout(0.2)(x)\n",
    "    # add a logistic layer -- let's say we have 6 classes\n",
    "    predictions = Dense(\n",
    "        1,\n",
    "        activation='sigmoid')(x)\n",
    "\n",
    "    # this is the model we will use\n",
    "    model = Model(\n",
    "        inputs=base_model.input,\n",
    "        outputs=predictions,\n",
    "    )\n",
    "\n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "    # i.e. freeze all base_model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # initiate an Adam optimizer\n",
    "    opt = Adam(\n",
    "        lr=learning_rate,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        decay=0.0,\n",
    "        amsgrad=False\n",
    "    )\n",
    "    \n",
    "    #K.clear_session()\n",
    "    # Let's train the model using Adam\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=[auc])\n",
    "    #K.clear_session()\n",
    "    \n",
    "    return base_model, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Generative Networks functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self,img_rows,img_cols,channels,latent_dim=100):\n",
    "        # Input shape\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        rr = int(round(self.img_rows/4))\n",
    "        cc = int(round(self.img_cols/4))\n",
    "        model.add(Dense(3 * rr * cc, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((rr, cc, 3)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self,X_train, epochs, name='aux', batch_size=128, save_interval=50):\n",
    "\n",
    "        self.name = name\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/\" + self.name + \"_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 112, 112, 32)      320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 56, 56, 64)        18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 57, 57, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 57, 57, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 57, 57, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 57, 57, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 29, 29, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 29, 29, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 29, 29, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 29, 29, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 29, 29, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 29, 29, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 29, 29, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 29, 29, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 215296)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 215297    \n",
      "=================================================================\n",
      "Total params: 604,929\n",
      "Trainable params: 604,033\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 9408)              950208    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 56, 56, 3)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 112, 112, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 112, 112, 128)     3584      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 112, 112, 128)     512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 112, 112, 128)     0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 224, 224, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 224, 224, 64)      73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 224, 224, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 224, 224, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 224, 224, 1)       577       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 224, 224, 1)       0         \n",
      "=================================================================\n",
      "Total params: 1,028,929\n",
      "Trainable params: 1,028,545\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = DCGAN(224, 224, 1)\n",
    "#(224, 224, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientid</th>\n",
       "      <th>index</th>\n",
       "      <th>SpecificCharacterSet</th>\n",
       "      <th>SOPClassUID</th>\n",
       "      <th>SOPInstanceUID</th>\n",
       "      <th>StudyDate</th>\n",
       "      <th>StudyTime</th>\n",
       "      <th>AccessionNumber</th>\n",
       "      <th>Modality</th>\n",
       "      <th>ConversionType</th>\n",
       "      <th>...</th>\n",
       "      <th>PixelSpacing</th>\n",
       "      <th>BitsAllocated</th>\n",
       "      <th>BitsStored</th>\n",
       "      <th>HighBit</th>\n",
       "      <th>PixelRepresentation</th>\n",
       "      <th>LossyImageCompression</th>\n",
       "      <th>LossyImageCompressionMethod</th>\n",
       "      <th>PixelData</th>\n",
       "      <th>filename</th>\n",
       "      <th>survival</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>362424</td>\n",
       "      <td>ISO_IR 100</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.12.2.1107.5.3.56.2693.11.202004071536250109</td>\n",
       "      <td>20200407</td>\n",
       "      <td>153625.000</td>\n",
       "      <td>ACC00009757</td>\n",
       "      <td>CR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 5549966 elements</td>\n",
       "      <td>362424.png</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>161101</td>\n",
       "      <td>ISO_IR 100</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.12.2.1107.5.3.56.2693.11.202003210459280593</td>\n",
       "      <td>20200321</td>\n",
       "      <td>45928.000</td>\n",
       "      <td>ACC00005107</td>\n",
       "      <td>CR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 6069570 elements</td>\n",
       "      <td>161101.png</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>161102</td>\n",
       "      <td>ISO_IR 100</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.12.2.1107.5.3.56.2693.11.202003260728100281</td>\n",
       "      <td>20200326</td>\n",
       "      <td>72810.000</td>\n",
       "      <td>ACC00005301</td>\n",
       "      <td>CR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 5683394 elements</td>\n",
       "      <td>161102.png</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>161103</td>\n",
       "      <td>ISO_IR 100</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.12.2.1107.5.3.56.2693.11.202003220726220593</td>\n",
       "      <td>20200322</td>\n",
       "      <td>72622.000</td>\n",
       "      <td>ACC00005441</td>\n",
       "      <td>CR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 5989198 elements</td>\n",
       "      <td>161103.png</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>161104</td>\n",
       "      <td>ISO_IR 100</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.12.2.1107.5.3.56.2693.11.202003221418280437</td>\n",
       "      <td>20200322</td>\n",
       "      <td>141828.000</td>\n",
       "      <td>ACC00005552</td>\n",
       "      <td>CR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 5990704 elements</td>\n",
       "      <td>161104.png</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3480</th>\n",
       "      <td>2278</td>\n",
       "      <td>460652</td>\n",
       "      <td>ISO_IR 100</td>\n",
       "      <td>Digital X-Ray Image Storage - For Presentation</td>\n",
       "      <td>1.3.46.670589.30.36.0.1.18774111139.1586556014...</td>\n",
       "      <td>20200410</td>\n",
       "      <td>225716.391</td>\n",
       "      <td>ACC00008725</td>\n",
       "      <td>DX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 5674014 elements</td>\n",
       "      <td>460652.png</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3481</th>\n",
       "      <td>2280</td>\n",
       "      <td>461518</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.51.0.7.11525005549.51456.28486.35726.61854...</td>\n",
       "      <td>20200415</td>\n",
       "      <td>103742.000</td>\n",
       "      <td>ACC00010093</td>\n",
       "      <td>CR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.15, 0.15]</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 5694582 elements</td>\n",
       "      <td>461518.png</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3482</th>\n",
       "      <td>2286</td>\n",
       "      <td>466280</td>\n",
       "      <td>ISO_IR 100</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.51.0.7.3142572467.2116.61512.35199.56756.9...</td>\n",
       "      <td>20200414</td>\n",
       "      <td>181443.000</td>\n",
       "      <td>ACC00009959</td>\n",
       "      <td>CR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.15, 0.15]</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 6762522 elements</td>\n",
       "      <td>466280.png</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3483</th>\n",
       "      <td>2287</td>\n",
       "      <td>466281</td>\n",
       "      <td>ISO_IR 100</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.51.0.7.12387810049.31749.41031.46011.30246...</td>\n",
       "      <td>20200415</td>\n",
       "      <td>95222.000</td>\n",
       "      <td>ACC00009454</td>\n",
       "      <td>CR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.1, 0.1]</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 19441046 elements</td>\n",
       "      <td>466281.png</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3484</th>\n",
       "      <td>2295</td>\n",
       "      <td>466290</td>\n",
       "      <td>ISO_IR 100</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.12.2.1107.5.3.56.2693.11.202004172341270250</td>\n",
       "      <td>20200417</td>\n",
       "      <td>234127.000</td>\n",
       "      <td>ACC00010320</td>\n",
       "      <td>CR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 5791876 elements</td>\n",
       "      <td>466290.png</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3485 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      patientid   index SpecificCharacterSet  \\\n",
       "0             1  362424           ISO_IR 100   \n",
       "1            13  161101           ISO_IR 100   \n",
       "2            13  161102           ISO_IR 100   \n",
       "3            13  161103           ISO_IR 100   \n",
       "4            13  161104           ISO_IR 100   \n",
       "...         ...     ...                  ...   \n",
       "3480       2278  460652           ISO_IR 100   \n",
       "3481       2280  461518                  NaN   \n",
       "3482       2286  466280           ISO_IR 100   \n",
       "3483       2287  466281           ISO_IR 100   \n",
       "3484       2295  466290           ISO_IR 100   \n",
       "\n",
       "                                         SOPClassUID  \\\n",
       "0                 Computed Radiography Image Storage   \n",
       "1                 Computed Radiography Image Storage   \n",
       "2                 Computed Radiography Image Storage   \n",
       "3                 Computed Radiography Image Storage   \n",
       "4                 Computed Radiography Image Storage   \n",
       "...                                              ...   \n",
       "3480  Digital X-Ray Image Storage - For Presentation   \n",
       "3481              Computed Radiography Image Storage   \n",
       "3482              Computed Radiography Image Storage   \n",
       "3483              Computed Radiography Image Storage   \n",
       "3484              Computed Radiography Image Storage   \n",
       "\n",
       "                                         SOPInstanceUID  StudyDate  \\\n",
       "0       1.3.12.2.1107.5.3.56.2693.11.202004071536250109   20200407   \n",
       "1       1.3.12.2.1107.5.3.56.2693.11.202003210459280593   20200321   \n",
       "2       1.3.12.2.1107.5.3.56.2693.11.202003260728100281   20200326   \n",
       "3       1.3.12.2.1107.5.3.56.2693.11.202003220726220593   20200322   \n",
       "4       1.3.12.2.1107.5.3.56.2693.11.202003221418280437   20200322   \n",
       "...                                                 ...        ...   \n",
       "3480  1.3.46.670589.30.36.0.1.18774111139.1586556014...   20200410   \n",
       "3481  1.3.51.0.7.11525005549.51456.28486.35726.61854...   20200415   \n",
       "3482  1.3.51.0.7.3142572467.2116.61512.35199.56756.9...   20200414   \n",
       "3483  1.3.51.0.7.12387810049.31749.41031.46011.30246...   20200415   \n",
       "3484    1.3.12.2.1107.5.3.56.2693.11.202004172341270250   20200417   \n",
       "\n",
       "       StudyTime AccessionNumber Modality  ConversionType  ...  PixelSpacing  \\\n",
       "0     153625.000     ACC00009757       CR             NaN  ...           NaN   \n",
       "1      45928.000     ACC00005107       CR             NaN  ...           NaN   \n",
       "2      72810.000     ACC00005301       CR             NaN  ...           NaN   \n",
       "3      72622.000     ACC00005441       CR             NaN  ...           NaN   \n",
       "4     141828.000     ACC00005552       CR             NaN  ...           NaN   \n",
       "...          ...             ...      ...             ...  ...           ...   \n",
       "3480  225716.391     ACC00008725       DX             NaN  ...           NaN   \n",
       "3481  103742.000     ACC00010093       CR             NaN  ...  [0.15, 0.15]   \n",
       "3482  181443.000     ACC00009959       CR             NaN  ...  [0.15, 0.15]   \n",
       "3483   95222.000     ACC00009454       CR             NaN  ...    [0.1, 0.1]   \n",
       "3484  234127.000     ACC00010320       CR             NaN  ...           NaN   \n",
       "\n",
       "     BitsAllocated BitsStored  HighBit  PixelRepresentation  \\\n",
       "0               16         12       11                    0   \n",
       "1               16         12       11                    0   \n",
       "2               16         12       11                    0   \n",
       "3               16         12       11                    0   \n",
       "4               16         12       11                    0   \n",
       "...            ...        ...      ...                  ...   \n",
       "3480            16         12       11                    0   \n",
       "3481            16         12       11                    0   \n",
       "3482            16         12       11                    0   \n",
       "3483            16         15       14                    0   \n",
       "3484            16         12       11                    0   \n",
       "\n",
       "      LossyImageCompression LossyImageCompressionMethod  \\\n",
       "0                         0                         NaN   \n",
       "1                         0                         NaN   \n",
       "2                         0                         NaN   \n",
       "3                         0                         NaN   \n",
       "4                         0                         NaN   \n",
       "...                     ...                         ...   \n",
       "3480                      0                         NaN   \n",
       "3481                      0                         NaN   \n",
       "3482                      0                         NaN   \n",
       "3483                      0                         NaN   \n",
       "3484                      0                         NaN   \n",
       "\n",
       "                       PixelData    filename survival  \n",
       "0      Array of 5549966 elements  362424.png        N  \n",
       "1      Array of 6069570 elements  161101.png        Y  \n",
       "2      Array of 5683394 elements  161102.png        Y  \n",
       "3      Array of 5989198 elements  161103.png        Y  \n",
       "4      Array of 5990704 elements  161104.png        Y  \n",
       "...                          ...         ...      ...  \n",
       "3480   Array of 5674014 elements  460652.png        Y  \n",
       "3481   Array of 5694582 elements  461518.png        Y  \n",
       "3482   Array of 6762522 elements  466280.png        Y  \n",
       "3483  Array of 19441046 elements  466281.png        Y  \n",
       "3484   Array of 5791876 elements  466290.png        Y  \n",
       "\n",
       "[3485 rows x 38 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_features = pd.read_csv(os.path.join(path_images, 'clean_data.csv'), sep=';');\n",
    "case_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3485, 38)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = 'survival'\n",
    "np.random.seed(seed)\n",
    "\n",
    "### Positive patient ids \n",
    "positive = case_features[case_features.survival.values == 'Y']\n",
    "positive_ids = positive.patientid.unique()\n",
    "\n",
    "### Negative patient ids \n",
    "negative = case_features[case_features.survival.values == 'N']\n",
    "negative_ids = negative.patientid.unique()\n",
    "n_negative_ids = len(negative_ids)\n",
    "\n",
    "### Test\n",
    "test_size = perc_split[2]*n_negative_ids\n",
    "test_ids = np.append(np.random.choice(positive_ids, size = math.floor(test_size), replace = False),\n",
    "                     np.random.choice(negative_ids, size = math.floor(test_size), replace = False))\n",
    "positive_ids = positive_ids[~np.isin(positive_ids, test_ids)]\n",
    "negative_ids = negative_ids[~np.isin(negative_ids, test_ids)]\n",
    "\n",
    "### Val\n",
    "val_size = perc_split[1]*n_negative_ids\n",
    "val_ids = np.append(np.random.choice(positive_ids, size = math.floor(val_size), replace = False),\n",
    "                     np.random.choice(negative_ids, size = math.floor(val_size), replace = False))\n",
    "positive_ids = positive_ids[~np.isin(positive_ids, val_ids)]\n",
    "negative_ids = negative_ids[~np.isin(negative_ids, val_ids)]\n",
    "\n",
    "### Train\n",
    "train_ids = np.append(positive_ids,\n",
    "                    negative_ids)\n",
    "\n",
    "# Split dataset based on patient ids\n",
    "case_features_train = case_features[case_features.patientid.isin(train_ids)]\n",
    "case_features_val = case_features[case_features.patientid.isin(val_ids)]\n",
    "case_features_test = case_features[case_features.patientid.isin(test_ids)]\n",
    "\n",
    "# Selección del patrón de datos X y del target y\n",
    "ytrain = case_features_train[target_class]\n",
    "del case_features_train[target_class]\n",
    "Xtrain = case_features_train\n",
    "\n",
    "yval = case_features_val[target_class]\n",
    "del case_features_val[target_class]\n",
    "Xval = case_features_val\n",
    "\n",
    "ytest = case_features_test[target_class]\n",
    "del case_features_test[target_class]\n",
    "Xtest = case_features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(case_features_train[ytrain == \"Y\"].patientid.unique()))\n",
    "print(len(case_features_train[ytrain == \"N\"].patientid.unique()))\n",
    "print(len(case_features_val[yval == \"Y\"].patientid.unique()))\n",
    "print(len(case_features_val[yval == \"N\"].patientid.unique()))\n",
    "print(len(case_features_test[ytest == \"Y\"].patientid.unique()))\n",
    "print(len(case_features_test[ytest == \"N\"].patientid.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(case_features_train.shape)\n",
    "print(case_features_val.shape)\n",
    "print(case_features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ytrain.value_counts())\n",
    "print(yval.value_counts())\n",
    "print(ytest.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for i in range(Xtrain.shape[0]):\n",
    "    print('%0.2f%%' % float(100*i/Xtrain.shape[0]))\n",
    "    image_path = os.path.join(path_images, 'processed', Xtrain.iloc[i].filename)\n",
    "    imagen = Image.open(image_path)\n",
    "    #imagen = np.asarray(imagen.convert(\"L\"))\n",
    "    #imagen = resize(imagen,  input_shape)\n",
    "    imagen = np.expand_dims(imagen, axis=-1)\n",
    "    imagen = resize(imagen,  input_shape)\n",
    "    X_train.append(imagen)\n",
    "\n",
    "X_train = np.stack(X_train, axis = 0)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = []\n",
    "for i in range(Xval.shape[0]):\n",
    "    #print('%0.2f%%' % float(100*i/Xval.shape[0]))\n",
    "    image_path = os.path.join(path_images, 'processed', Xval.iloc[i].filename)\n",
    "    imagen = Image.open(image_path)\n",
    "    #imagen = np.asarray(imagen.convert(\"L\"))\n",
    "    #imagen = resize(imagen,  input_shape)\n",
    "    imagen = np.expand_dims(imagen, axis=-1)\n",
    "    imagen = resize(imagen,  input_shape)\n",
    "    X_val.append(imagen)\n",
    "\n",
    "X_val = np.stack(X_val, axis = 0)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for i in range(Xtest.shape[0]):\n",
    "    image_path = os.path.join(path_images, 'processed', Xtest.iloc[i].filename)\n",
    "    imagen = Image.open(image_path)\n",
    "    #imagen = np.asarray(imagen.convert(\"L\"))\n",
    "    #imagen = resize(imagen,  input_shape)\n",
    "    imagen = np.expand_dims(imagen, axis=-1)\n",
    "    imagen = resize(imagen,  input_shape)\n",
    "    X_test.append(imagen)\n",
    "\n",
    "X_test = np.stack(X_test, axis = 0)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_test', X_test)\n",
    "np.save('X_train', X_train)\n",
    "np.save('X_val', X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open(\"filename\",\"w+\")\n",
    "X_test = np.load('X_test.npy')\n",
    "X_train = np.load('X_train.npy')\n",
    "X_val = np.load('X_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107, 224, 224, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "0 [D loss: 7.159954, acc.: 28.12%] [G loss: 0.011982]\n",
      "1 [D loss: 0.335417, acc.: 78.12%] [G loss: 7.918631]\n",
      "2 [D loss: 2.769427, acc.: 43.36%] [G loss: 0.656274]\n",
      "3 [D loss: 1.262107, acc.: 50.00%] [G loss: 8.956099]\n",
      "4 [D loss: 1.493726, acc.: 55.47%] [G loss: 1.893862]\n",
      "5 [D loss: 1.063522, acc.: 49.22%] [G loss: 8.405457]\n",
      "6 [D loss: 0.866453, acc.: 80.47%] [G loss: 6.782518]\n",
      "7 [D loss: 0.529113, acc.: 91.02%] [G loss: 1.628308]\n",
      "8 [D loss: 0.741941, acc.: 73.83%] [G loss: 4.133011]\n",
      "9 [D loss: 0.610749, acc.: 91.41%] [G loss: 4.563258]\n",
      "10 [D loss: 0.631758, acc.: 91.80%] [G loss: 0.782184]\n",
      "11 [D loss: 0.844776, acc.: 60.16%] [G loss: 6.812692]\n",
      "12 [D loss: 0.896310, acc.: 83.59%] [G loss: 3.987325]\n",
      "13 [D loss: 0.756626, acc.: 91.41%] [G loss: 0.399868]\n",
      "14 [D loss: 0.491749, acc.: 77.34%] [G loss: 5.338595]\n",
      "15 [D loss: 0.470576, acc.: 91.80%] [G loss: 5.302586]\n",
      "16 [D loss: 0.557269, acc.: 89.45%] [G loss: 0.533932]\n",
      "17 [D loss: 0.932746, acc.: 56.64%] [G loss: 10.131482]\n",
      "18 [D loss: 1.261043, acc.: 71.88%] [G loss: 1.806324]\n",
      "19 [D loss: 0.650932, acc.: 73.83%] [G loss: 7.647220]\n",
      "20 [D loss: 0.619472, acc.: 90.62%] [G loss: 7.183894]\n",
      "21 [D loss: 0.494249, acc.: 93.36%] [G loss: 2.349533]\n",
      "22 [D loss: 0.662524, acc.: 72.27%] [G loss: 8.635636]\n",
      "23 [D loss: 0.750646, acc.: 82.81%] [G loss: 3.279664]\n",
      "24 [D loss: 0.398288, acc.: 87.50%] [G loss: 4.421299]\n",
      "25 [D loss: 0.338010, acc.: 91.41%] [G loss: 2.740856]\n",
      "26 [D loss: 0.211443, acc.: 92.97%] [G loss: 6.856164]\n",
      "27 [D loss: 0.352863, acc.: 93.36%] [G loss: 3.807774]\n",
      "28 [D loss: 0.198845, acc.: 92.19%] [G loss: 8.882738]\n",
      "29 [D loss: 0.520458, acc.: 82.42%] [G loss: 5.684390]\n",
      "30 [D loss: 0.151332, acc.: 96.09%] [G loss: 5.565250]\n",
      "31 [D loss: 0.174036, acc.: 95.70%] [G loss: 6.395851]\n",
      "32 [D loss: 0.283106, acc.: 95.31%] [G loss: 4.100859]\n",
      "33 [D loss: 0.291753, acc.: 89.45%] [G loss: 9.013900]\n",
      "34 [D loss: 2.304388, acc.: 32.42%] [G loss: 20.932907]\n",
      "35 [D loss: 4.509527, acc.: 49.61%] [G loss: 0.093127]\n",
      "36 [D loss: 3.989056, acc.: 48.44%] [G loss: 19.597355]\n",
      "37 [D loss: 1.428591, acc.: 79.69%] [G loss: 23.332405]\n",
      "38 [D loss: 1.662290, acc.: 78.12%] [G loss: 4.694305]\n",
      "39 [D loss: 0.818184, acc.: 78.52%] [G loss: 5.747087]\n",
      "40 [D loss: 0.642974, acc.: 90.62%] [G loss: 5.675850]\n",
      "41 [D loss: 0.375905, acc.: 95.70%] [G loss: 2.728862]\n",
      "42 [D loss: 0.325311, acc.: 93.36%] [G loss: 0.500969]\n",
      "43 [D loss: 0.341039, acc.: 91.02%] [G loss: 2.398265]\n",
      "44 [D loss: 0.264057, acc.: 95.31%] [G loss: 2.792511]\n",
      "45 [D loss: 0.657107, acc.: 84.38%] [G loss: 6.217828]\n",
      "46 [D loss: 0.228674, acc.: 95.31%] [G loss: 5.565273]\n",
      "47 [D loss: 0.144479, acc.: 96.48%] [G loss: 3.354456]\n",
      "48 [D loss: 0.833362, acc.: 69.53%] [G loss: 16.405560]\n",
      "49 [D loss: 1.611720, acc.: 76.56%] [G loss: 6.098632]\n",
      "50 [D loss: 0.015340, acc.: 99.61%] [G loss: 1.247543]\n",
      "51 [D loss: 0.107395, acc.: 95.70%] [G loss: 2.604510]\n",
      "52 [D loss: 0.064230, acc.: 98.83%] [G loss: 2.528479]\n",
      "53 [D loss: 0.090092, acc.: 98.83%] [G loss: 1.767174]\n",
      "54 [D loss: 0.086892, acc.: 98.05%] [G loss: 2.857905]\n",
      "55 [D loss: 0.288627, acc.: 89.06%] [G loss: 9.891235]\n",
      "56 [D loss: 0.402493, acc.: 91.80%] [G loss: 3.468060]\n",
      "57 [D loss: 0.990310, acc.: 62.89%] [G loss: 22.318106]\n",
      "58 [D loss: 6.651562, acc.: 9.38%] [G loss: 11.453011]\n",
      "59 [D loss: 0.459852, acc.: 89.06%] [G loss: 15.083313]\n",
      "60 [D loss: 0.692043, acc.: 85.55%] [G loss: 1.720913]\n",
      "61 [D loss: 1.210902, acc.: 57.42%] [G loss: 22.416370]\n",
      "62 [D loss: 3.250000, acc.: 55.47%] [G loss: 0.215268]\n",
      "63 [D loss: 2.367553, acc.: 49.61%] [G loss: 25.549852]\n",
      "64 [D loss: 2.369931, acc.: 63.67%] [G loss: 12.366332]\n",
      "65 [D loss: 0.217684, acc.: 96.09%] [G loss: 4.631659]\n",
      "66 [D loss: 0.564348, acc.: 79.69%] [G loss: 10.892500]\n",
      "67 [D loss: 0.533418, acc.: 89.84%] [G loss: 9.047084]\n",
      "68 [D loss: 0.115771, acc.: 97.66%] [G loss: 3.669514]\n",
      "69 [D loss: 0.213644, acc.: 89.84%] [G loss: 5.849318]\n",
      "70 [D loss: 0.233265, acc.: 94.53%] [G loss: 4.226927]\n",
      "71 [D loss: 0.392927, acc.: 88.67%] [G loss: 4.701671]\n",
      "72 [D loss: 0.294958, acc.: 92.97%] [G loss: 2.246649]\n",
      "73 [D loss: 0.653877, acc.: 73.83%] [G loss: 12.305644]\n",
      "74 [D loss: 3.777387, acc.: 14.45%] [G loss: 18.153324]\n",
      "75 [D loss: 2.148990, acc.: 64.06%] [G loss: 2.820151]\n",
      "76 [D loss: 1.003971, acc.: 69.53%] [G loss: 6.935369]\n",
      "77 [D loss: 0.288542, acc.: 96.88%] [G loss: 16.842403]\n",
      "78 [D loss: 0.395384, acc.: 90.23%] [G loss: 1.547006]\n",
      "79 [D loss: 0.189189, acc.: 98.05%] [G loss: 0.031377]\n",
      "80 [D loss: 0.028348, acc.: 99.61%] [G loss: 0.004359]\n",
      "81 [D loss: 0.243024, acc.: 93.36%] [G loss: 0.089664]\n",
      "82 [D loss: 0.007860, acc.: 99.61%] [G loss: 0.721368]\n",
      "83 [D loss: 0.095828, acc.: 94.92%] [G loss: 3.991997]\n",
      "84 [D loss: 0.194208, acc.: 96.09%] [G loss: 6.380860]\n",
      "85 [D loss: 0.429274, acc.: 79.30%] [G loss: 19.311695]\n"
     ]
    }
   ],
   "source": [
    "model.train(X_train, epochs=2000, name='covid_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain[ytrain=='Y'] = 0\n",
    "ytrain[ytrain=='N'] = 1\n",
    "y_train = np.array(ytrain, dtype = np.int64)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yval[yval=='Y'] = 0\n",
    "yval[yval=='N'] = 1\n",
    "y_val = np.array(yval, dtype = np.int64)\n",
    "y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest[ytest=='Y'] = 0\n",
    "ytest[ytest=='N'] = 1\n",
    "y_test = np.array(ytest, dtype = np.int64)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set class weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_pos = np.count_nonzero(y_train == 0) / len(y_train)\n",
    "ratio_neg = np.count_nonzero(y_train == 1) / len(y_train)\n",
    "class_weight_train = np.array((ratio_pos, ratio_neg))\n",
    "print(class_weight_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_pos = np.count_nonzero(y_val == 0) / len(y_val)\n",
    "ratio_neg = np.count_nonzero(y_val == 1) / len(y_val)\n",
    "class_weight_val = np.array((ratio_pos, ratio_neg))\n",
    "print(class_weight_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_pos = np.count_nonzero(y_test == 0) / len(y_test)\n",
    "ratio_neg = np.count_nonzero(y_test == 1) / len(y_test)\n",
    "class_weight_test = np.array((ratio_pos, ratio_neg))\n",
    "print(class_weight_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=True, \n",
    "                             featurewise_std_normalization=True, \n",
    "                             rotation_range=90)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(\n",
    "    os.getcwd(),\n",
    "    '../saved_models'\n",
    ")\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# This callback saves the weights of the model after each epoch\n",
    "checkpoint = ModelCheckpoint(\n",
    "    '../saved_models/weights.epoch_{epoch:02d}.hdf5',\n",
    "    monitor='val_loss', \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# This callback writes logs for TensorBoard\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir='./Graph', \n",
    "    histogram_freq=0,  \n",
    "    write_graph=True\n",
    ")\n",
    "\n",
    "# This callback writes logs for TensorBoard\n",
    "tensorboard_augmented = TensorBoard(\n",
    "    log_dir='./Graph_augmented', \n",
    "    histogram_freq=0,  \n",
    "    write_graph=True\n",
    ")\n",
    "\n",
    "my_callbacks = EarlyStopping(monitor='val_auc', patience=200, verbose=1, mode='max')\n",
    "my_callbacks_augmented = EarlyStopping(monitor='val_auc', patience=100, verbose=1, mode='max')\n",
    "\n",
    "\n",
    "callbacks_list = [tensorboard, my_callbacks]\n",
    "callbacks_list_augmented = [tensorboard_augmented, my_callbacks_augmented]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=True, \n",
    "                             featurewise_std_normalization=True, \n",
    "                             rotation_range=90)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CNN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed\n",
    "base_model, model = get_model(learning_rate)\n",
    "\n",
    "# Show layers\n",
    "print_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read created data by GANs\n",
    "\n",
    "import random\n",
    "\n",
    "previous_val_auc = 0\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for n in size_gans:\n",
    "    print (\"Numero imagenes GANs %d\" % n)\n",
    "    \n",
    "    X_train = np.load(os.path.join(path_images, 'X_train.npy'))\n",
    "    X_val = np.load(os.path.join(path_images, 'X_val.npy'))\n",
    "    X_test = np.load(os.path.join(path_images, 'X_test.npy'))\n",
    "    y_train = np.load(os.path.join(path_images, 'y_train.npy'))\n",
    "    y_val = np.load(os.path.join(path_images, 'y_val.npy'))\n",
    "    y_test = np.load(os.path.join(path_images, 'y_test.npy'))\n",
    "    \n",
    "    if n != 0:\n",
    "        X_train_0_gans = np.load(os.path.join(path_gans, 'covid_0_num_imag_%d.npy' % n))\n",
    "        X_train_1_gans = np.load(os.path.join(path_gans, 'covid_1_num_imag_%d.npy'% n))\n",
    "\n",
    "        ratio_neg = np.count_nonzero(y_train == 1) / len(y_train)\n",
    "        size = round(ratio_neg * X_train_1_gans.shape[0])\n",
    "        lista = range(X_train_1_gans.shape[0]-1)\n",
    "\n",
    "\n",
    "        ind = random.sample(lista, size)\n",
    "        X_train_1_gans = X_train_1_gans[ind,:]\n",
    "\n",
    "        # create target\n",
    "        y_train_1_gans = np.ones(X_train_1_gans.shape[0], dtype=int)\n",
    "        y_train_0_gans = np.zeros(X_train_0_gans.shape[0], dtype=int)\n",
    "\n",
    "\n",
    "        # join data\n",
    "\n",
    "        X_train_gans = np.concatenate((X_train_1_gans, X_train_0_gans))\n",
    "        y_train_gans = np.concatenate((y_train_1_gans, y_train_0_gans))\n",
    "\n",
    "        X_train = np.concatenate((X_train, X_train_gans))\n",
    "        y_train = np.concatenate((y_train, y_train_gans))\n",
    "    \n",
    "    \n",
    "    # train\n",
    "    ratio_pos = np.count_nonzero(y_train == 0) / len(y_train)\n",
    "    ratio_neg = np.count_nonzero(y_train == 1) / len(y_train)\n",
    "    class_weight_train = np.array((ratio_pos, ratio_neg))\n",
    "\n",
    "    # val\n",
    "    ratio_pos = np.count_nonzero(y_val == 0) / len(y_val)\n",
    "    ratio_neg = np.count_nonzero(y_val == 1) / len(y_val)\n",
    "    class_weight_val = np.array((ratio_pos, ratio_neg))\n",
    "\n",
    "    \n",
    "    out = model.fit(X_train, y_train,\n",
    "                     validation_data=(X_val, y_val),\n",
    "                     steps_per_epoch=len(X_train) / batch_size, \n",
    "                     epochs=epochs,\n",
    "                     class_weight=class_weight_train,\n",
    "                     callbacks = callbacks_list,       \n",
    "                     verbose=1)\n",
    "    \n",
    "    if(len(out.history)):\n",
    "        acum_tr_auc.append(out.history['auc'][0])\n",
    "        acum_val_auc.append(out.history['val_auc'][0])\n",
    "        acum_tr_loss.append(out.history['loss'][0])\n",
    "        acum_val_loss.append(out.history['val_loss'][0])\n",
    "                \n",
    "        acum = pd.DataFrame([acum_tr_auc, acum_val_auc, acum_tr_loss, acum_val_loss])\n",
    "        acum.to_csv(os.path.join(path_results,'acum_results_no_augmentation.csv'))\n",
    "                    \n",
    "        #if len(acum_tr_auc) > 1:\n",
    "        clear_output()\n",
    "        best_i = np.argmax(acum_val_auc)\n",
    "        grafica_entrenamiento(acum_tr_auc, acum_val_auc, acum_tr_loss, acum_val_loss, best_i)\n",
    "            ### save loss and auc of train and val     \n",
    "        stopped_epoch = early_stopping.stopped_epoch\n",
    "        train_loss = out.history['loss'][stopped_epoch-1]\n",
    "        val_loss = out.history['val_loss'][stopped_epoch-1]\n",
    "        train_auc = out.history['auc'][stopped_epoch-1]\n",
    "        val_auc = out.history['val_auc'][stopped_epoch-1]\n",
    "        model = out.model\n",
    "\n",
    "        pred_train = model.predict(X_train)\n",
    "        pred_val = model.predict(X_val)\n",
    "\n",
    "        train_auc = roc_auc_score(y_true = y_train, y_score = pred_train)\n",
    "        val_auc = roc_auc_score(y_true = y_val, y_score = pred_val)\n",
    "\n",
    "        \n",
    "        res = pd.DataFrame([n, epochs, batch_size, stopped_epoch, train_auc, val_auc])\n",
    "        df = pd.concat([df, res], axis=1)\n",
    "\n",
    "        df.to_csv(os.path.join(path_results,'model_results_augmentation.csv')) \n",
    "                \n",
    "        model.save(os.path.join(path_results, 'model_augmentation_%d.h5' % n))\n",
    "            \n",
    "        if(previous_val_auc < val_auc):\n",
    "            save_dir = os.path.join(\n",
    "                    os.getcwd(),\n",
    "                    '../model_results_y')\n",
    "            if not os.path.isdir(save_dir):\n",
    "                    os.makedirs(save_dir)\n",
    "                model.save(os.path.join(path_results, 'model_no_augmentation_%d.h5' % n))\n",
    "                \n",
    "            previous_val_auc = val_auc\n",
    "                \n",
    "\n",
    "                           \n",
    "df.index = ['size_gans', 'epochs', 'batch_size','early_stopping', 'train_auc', 'val_auc']\n",
    "df.to_csv(os.path.join(path_results,'model_results_augmentation.csv'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read created data by GANs\n",
    "\n",
    "import random\n",
    "\n",
    "previous_val_auc = 0\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for n in size_gans:\n",
    "    print (\"Numero imagenes GANs %d\" % n)\n",
    "    \n",
    "    X_train = np.load(os.path.join(path_images, 'X_train.npy'))\n",
    "    X_val = np.load(os.path.join(path_images, 'X_val.npy'))\n",
    "    X_test = np.load(os.path.join(path_images, 'X_test.npy'))\n",
    "    y_train = np.load(os.path.join(path_images, 'y_train.npy'))\n",
    "    y_val = np.load(os.path.join(path_images, 'y_val.npy'))\n",
    "    y_test = np.load(os.path.join(path_images, 'y_test.npy'))\n",
    "    \n",
    "    if n != 0:\n",
    "        X_train_0_gans = np.load(os.path.join(path_gans, 'covid_0_num_imag_%d.npy' % n))\n",
    "        X_train_1_gans = np.load(os.path.join(path_gans, 'covid_1_num_imag_%d.npy'% n))\n",
    "\n",
    "        ratio_neg = np.count_nonzero(y_train == 1) / len(y_train)\n",
    "        size = round(ratio_neg * X_train_1_gans.shape[0])\n",
    "        lista = range(X_train_1_gans.shape[0]-1)\n",
    "\n",
    "\n",
    "        ind = random.sample(lista, size)\n",
    "        X_train_1_gans = X_train_1_gans[ind,:]\n",
    "\n",
    "        # create target\n",
    "        y_train_1_gans = np.ones(X_train_1_gans.shape[0], dtype=int)\n",
    "        y_train_0_gans = np.zeros(X_train_0_gans.shape[0], dtype=int)\n",
    "\n",
    "\n",
    "        # join data\n",
    "\n",
    "        X_train_gans = np.concatenate((X_train_1_gans, X_train_0_gans))\n",
    "        y_train_gans = np.concatenate((y_train_1_gans, y_train_0_gans))\n",
    "\n",
    "        X_train = np.concatenate((X_train, X_train_gans))\n",
    "        y_train = np.concatenate((y_train, y_train_gans))\n",
    "    \n",
    "    \n",
    "    # train\n",
    "    ratio_pos = np.count_nonzero(y_train == 0) / len(y_train)\n",
    "    ratio_neg = np.count_nonzero(y_train == 1) / len(y_train)\n",
    "    class_weight_train = np.array((ratio_pos, ratio_neg))\n",
    "\n",
    "    # val\n",
    "    ratio_pos = np.count_nonzero(y_val == 0) / len(y_val)\n",
    "    ratio_neg = np.count_nonzero(y_val == 1) / len(y_val)\n",
    "    class_weight_val = np.array((ratio_pos, ratio_neg))\n",
    "\n",
    "    datagen = ImageDataGenerator(featurewise_center=True, \n",
    "                             featurewise_std_normalization=True,\n",
    "                             rotation_range=90,\n",
    "                             brightness_range = (0.25, 0.75))\n",
    "    datagen.fit(X_train)\n",
    "    \n",
    "    base_model_augmented, model_augmented = get_model(learning_rate)\n",
    "    \n",
    "    out = model_augmented.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size, seed=seed),\n",
    "                     validation_data=(X_val, y_val),\n",
    "                     steps_per_epoch=len(X_train) / batch_size, \n",
    "                     epochs=epochs,\n",
    "                     class_weight=class_weight_train,\n",
    "                     callbacks = callbacks_list,       \n",
    "                     verbose=1)\n",
    "    \n",
    "    if(len(out.history)):\n",
    "        acum_tr_auc.append(out.history['auc'][0])\n",
    "        acum_val_auc.append(out.history['val_auc'][0])\n",
    "        acum_tr_loss.append(out.history['loss'][0])\n",
    "        acum_val_loss.append(out.history['val_loss'][0])\n",
    "                \n",
    "        acum = pd.DataFrame([acum_tr_auc, acum_val_auc, acum_tr_loss, acum_val_loss])\n",
    "        acum.to_csv(os.path.join(path_results,'acum_results_no_augmentation.csv'))\n",
    "                    \n",
    "        #if len(acum_tr_auc) > 1:\n",
    "        clear_output()\n",
    "        best_i = np.argmax(acum_val_auc)\n",
    "        grafica_entrenamiento(acum_tr_auc, acum_val_auc, acum_tr_loss, acum_val_loss, best_i)\n",
    "            ### save loss and auc of train and val     \n",
    "        stopped_epoch = early_stopping.stopped_epoch\n",
    "        train_loss = out.history['loss'][stopped_epoch-1]\n",
    "        val_loss = out.history['val_loss'][stopped_epoch-1]\n",
    "        train_auc = out.history['auc'][stopped_epoch-1]\n",
    "        val_auc = out.history['val_auc'][stopped_epoch-1]\n",
    "        model = out.model\n",
    "\n",
    "        pred_train = model.predict(X_train)\n",
    "        pred_val = model.predict(X_val)\n",
    "\n",
    "        train_auc = roc_auc_score(y_true = y_train, y_score = pred_train)\n",
    "        val_auc = roc_auc_score(y_true = y_val, y_score = pred_val)\n",
    "\n",
    "        \n",
    "        res = pd.DataFrame([n, epochs, batch_size, stopped_epoch, train_auc, val_auc])\n",
    "        df = pd.concat([df, res], axis=1)\n",
    "\n",
    "        df.to_csv(os.path.join(path_results,'model_results_augmentation.csv')) \n",
    "                \n",
    "        model.save(os.path.join(path_results, 'model_augmentation_%d.h5' % n))\n",
    "            \n",
    "        if(previous_val_auc < val_auc):\n",
    "            save_dir = os.path.join(\n",
    "                    os.getcwd(),\n",
    "                    '../model_results_y')\n",
    "            if not os.path.isdir(save_dir):\n",
    "                    os.makedirs(save_dir)\n",
    "                model.save(os.path.join(path_results, 'model_no_augmentation_%d.h5' % n))\n",
    "                \n",
    "            previous_val_auc = val_auc\n",
    "                \n",
    "\n",
    "                           \n",
    "df.index = ['size_gans', 'epochs', 'batch_size','early_stopping', 'train_auc', 'val_auc']\n",
    "df.to_csv(os.path.join(path_results,'model_results_augmentation.csv'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train)\n",
    "pred_val = model.predict(X_val)\n",
    "pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_train = roc_auc_score(y_true = y_train, y_score = pred_train)\n",
    "auc_val = roc_auc_score(y_true = y_val, y_score = pred_val)\n",
    "auc_test = roc_auc_score(y_true = y_test, y_score = pred_test)\n",
    "print('AUC train = %s - AUC val = %s - AUC test = %s' % (str(auc_train), str(auc_val), str(auc_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels_train = (pred_train >= 0.5).astype(int)\n",
    "y_labels_val = (pred_val >= 0.5).astype(int)\n",
    "y_labels_test = (pred_test >= 0.5).astype(int)\n",
    "cm_train = confusion_matrix(y_pred = y_labels_train, y_true = y_train)\n",
    "cm_val = confusion_matrix(y_pred = y_labels_val, y_true = y_val)\n",
    "cm_test = confusion_matrix(y_pred = y_labels_test, y_true = y_test)\n",
    "print(cm_train)\n",
    "print(cm_val)\n",
    "print(cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fpr_train, tpr_train, threshold_train = roc_curve(y_train, pred_train)\n",
    "roc_auc_train = roc_auc_score(y_true = y_train, y_score = pred_train)\n",
    "fpr_val, tpr_val, threshold_val = roc_curve(y_val, pred_val)\n",
    "roc_auc_val = roc_auc_score(y_true = y_val, y_score = pred_val)\n",
    "fpr_test, tpr_test, threshold_test = roc_curve(y_test, pred_test)\n",
    "roc_auc_test = roc_auc_score(y_true = y_test, y_score = pred_test)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr_train, tpr_train, 'r', label = 'AUC = %0.2f' % roc_auc_test)\n",
    "plt.plot(fpr_val, tpr_val, 'g', label = 'AUC = %0.2f' % roc_auc_val)\n",
    "plt.plot(fpr_test, tpr_test, 'b', label = 'AUC = %0.2f' % roc_auc_test)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.xlim([-0.01, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr_train, tpr_train, 'b', label = 'AUC = %0.2f' % roc_auc_train)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([-0.01, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(os.path.join('../predictions_y', 'X_train.csv'))\n",
    "np.savetxt(os.path.join('../predictions_y', 'predictions_train.csv'), pred_train, delimiter=\";\")\n",
    "np.savetxt(os.path.join('../predictions_y', 'y_train.csv'), y_train, delimiter=\";\")\n",
    "X_val.to_csv(os.path.join('../predictions_y', 'X_val.csv'))\n",
    "np.savetxt(os.path.join('../predictions_y', 'predictions_val.csv'), pred_val, delimiter=\";\")\n",
    "np.savetxt(os.path.join('../predictions_y', 'y_val.csv'), y_val, delimiter=\";\")\n",
    "X_test.to_csv(os.path.join('../predictions_y', 'X_test.csv'))\n",
    "np.savetxt(os.path.join('../predictions_y', 'predictions_test.csv'),pred_test, delimiter=\";\")\n",
    "np.savetxt(os.path.join('../predictions_y', 'y_test.csv'), y_test, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_CI_train = bootstrap_auc(y_train, pred_train, bootstraps = 100, fold_size = 1000,)\n",
    "AUC_CI_val = bootstrap_auc(y_val, pred_val, bootstraps = 100, fold_size = 1000)\n",
    "AUC_CI_test = bootstrap_auc(y_test, pred_test, bootstraps = 100, fold_size = 1000,)\n",
    "AUC_CI = print_confidence_intervals(AUC_CI_train,)\n",
    "AUC_CI = AUC_CI.append(print_confidence_intervals(AUC_CI_val), ignore_index=True)\n",
    "AUC_CI = AUC_CI.append(print_confidence_intervals(AUC_CI_test), ignore_index=True)\n",
    "AUC_CI.index = ['Train', 'Val', 'Test'];\n",
    "AUC_CI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_train, tpr_train, threshold_train = roc_curve(y_train, pred_train)\n",
    "roc_auc_train = roc_auc_score(y_true = y_train, y_score = pred_train)\n",
    "fpr_val, tpr_val, threshold_val = roc_curve(y_val, pred_val)\n",
    "roc_auc_val = roc_auc_score(y_true = y_val, y_score = pred_val)\n",
    "fpr_test, tpr_test, threshold_test = roc_curve(y_test, pred_test)\n",
    "roc_auc_test = roc_auc_score(y_true = y_test, y_score = pred_test)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr_train, tpr_train, 'r', label = 'AUC = %0.2f' % roc_auc_train)\n",
    "plt.plot(fpr_val, tpr_val, 'g', label = 'AUC = %0.2f' % roc_auc_val)\n",
    "plt.plot(fpr_test, tpr_test, 'b', label = 'AUC = %0.2f' % roc_auc_test)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap / Gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.where(y_test == 1)[0]:\n",
    "    print('index ' + str(i));\n",
    "    heat_map, superimposed_img = show_heatmap(model, X_test[i], y_test[i], pred_test[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
