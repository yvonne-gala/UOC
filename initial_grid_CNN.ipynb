{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Generic\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Images\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "# import talos as ta\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, plot_confusion_matrix, roc_curve\n",
    "\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "\n",
    "# Keras\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.utils import print_summary\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.applications.densenet import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = '/home/ygala/TFM_UOC/'\n",
    "path_images = os.path.join(path_root, 'data', 'covid_images')\n",
    "path_results = os.path.join(os.getcwd(),'final_model_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_split = [0.7, 0.15, 0.15]\n",
    "input_shape = (224, 224, 3)\n",
    "seed = 14\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Classes and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CheXnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chexNet weights\n",
    "chexnet_weights = '/home/ygala/TFM_UOC/scripts/chexnet/best_weights.h5'\n",
    "\n",
    "def chexnet_preprocess_input(value):\n",
    "    return preprocess_input(value)\n",
    "\n",
    "\n",
    "def get_chexnet_model():\n",
    "    input_shape = (224, 224, 3)\n",
    "    img_input = Input(shape=input_shape)\n",
    "    base_weights = 'imagenet'\n",
    "\n",
    "    # create the base pre-trained model\n",
    "    base_model = DenseNet121(\n",
    "        include_top=False,\n",
    "        input_tensor=img_input,\n",
    "        input_shape=input_shape,\n",
    "        weights=base_weights,\n",
    "        pooling='avg'\n",
    "    )\n",
    "\n",
    "    x = base_model.output\n",
    "    # add a logistic layer -- let's say we have 14 classes\n",
    "    predictions = Dense(\n",
    "        14,\n",
    "        activation='sigmoid',\n",
    "        name='predictions')(x)\n",
    "\n",
    "    # this is the model we will use\n",
    "    model = Model(\n",
    "        inputs=img_input,\n",
    "        outputs=predictions,\n",
    "    )\n",
    "\n",
    "    # load chexnet weights\n",
    "    model.load_weights(chexnet_weights)\n",
    "\n",
    "    # return model\n",
    "    return base_model, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weight(csv_file_path, target_class):\n",
    "    df = pd.read_csv(csv_file_path, sep=';')\n",
    "    total_counts = df.shape[0]\n",
    "    class_weight = []\n",
    "\n",
    "    ratio_pos = df.loc[(df[target_class] == 'Y')].shape[0] / total_counts\n",
    "    ratio_neg = df.loc[(df[target_class] == 'N')].shape[0] / total_counts\n",
    "    class_weight = np.array((ratio_pos, ratio_neg))\n",
    "        \n",
    "    return class_weight\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "\n",
    "def print_confidence_intervals(statistics):\n",
    "    df = pd.DataFrame(columns=[\"Mean AUC (CI 5%-95%)\"])\n",
    "    mean = statistics.mean()\n",
    "    max_ = np.quantile(statistics, .95)\n",
    "    min_ = np.quantile(statistics, .05)\n",
    "    df.loc[\"Exitus\"] = [\"%.2f (%.2f-%.2f)\" % (mean, min_, max_)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_model(learning_rate):\n",
    "    # get base model, model\n",
    "    base_model, chexnet_model = get_chexnet_model()\n",
    "\n",
    "    x = base_model.output\n",
    "    \n",
    "    # Regularization layer\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Dense layer\n",
    "    x = Dense(128, \n",
    "              activation='relu',\n",
    "              kernel_regularizer=tf.keras.regularizers.l1_l2(0.5, 0.0001))(x)\n",
    "    \n",
    "    # Regularization layer\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    \n",
    "    # add a logistic layer -- let's say we have 6 classes\n",
    "    predictions = Dense(\n",
    "        1,\n",
    "        activation='sigmoid')(x)\n",
    "\n",
    "    # this is the model we will use\n",
    "    model = Model(\n",
    "        inputs=base_model.input,\n",
    "        outputs=predictions,\n",
    "    )\n",
    "\n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # initiate an Adam optimizer\n",
    "    opt = Adam(\n",
    "        lr=learning_rate,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        decay=0.0,\n",
    "        amsgrad=False\n",
    "    )\n",
    "\n",
    "    # Let's train the model using Adam\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=[metrics.BinaryAccuracy(name = \"acc\"),\n",
    "                metrics.AUC(name = \"auc\")])\n",
    "\n",
    "    return base_model, model\n",
    "\n",
    "\n",
    "class print_learning_rate(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        print(f'Learning rate = {K.eval(lr):.5f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grafica_entrenamiento(tr_auc, val_auc, tr_loss, val_loss, best_i,\n",
    "                          figsize=(10,5), path_results = None):\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = plt.subplot(1,2,1)\n",
    "    plt.plot(1+np.arange(len(tr_loss)), np.array(tr_loss))\n",
    "    plt.plot(1+np.arange(len(val_loss)), np.array(val_loss))\n",
    "    plt.plot(1+best_i, val_loss[best_i], 'or')\n",
    "    plt.title('loss del modelo', fontsize=18)\n",
    "    plt.ylabel('loss', fontsize=12)\n",
    "    plt.xlabel('época', fontsize=18)        \n",
    "    plt.legend(['entrenamiento', 'validación'], loc='upper left')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    \n",
    "    plt.plot(1+np.arange(len(tr_auc)),  np.array(tr_auc))\n",
    "    plt.plot(1+np.arange(len(val_auc)), np.array(val_auc))\n",
    "    plt.plot(1+best_i, val_auc[best_i], 'or')\n",
    "    plt.title('AUC', fontsize=18)\n",
    "    plt.ylabel('AUC', fontsize=12)\n",
    "    plt.xlabel('época', fontsize=18)    \n",
    "    plt.legend(['entrenamiento', 'validación'], loc='upper left')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    if (path_results != None):\n",
    "        plt.savefig(os.path.join(path_results, 'auc_loss.png'))\n",
    "    plt.show()\n",
    "    \n",
    "class TrainingPlot(Callback):\n",
    "    \n",
    "    # This function is called when the training begins\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # Initialize the lists for holding the logs, losses and accuracies\n",
    "        self.losses = []\n",
    "        self.auc = []\n",
    "        self.val_losses = []\n",
    "        self.val_auc = []\n",
    "        self.logs = []\n",
    "    \n",
    "    # This function is called at the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):       \n",
    "       \n",
    "        \n",
    "        # Before plotting ensure at least 10 epochs have passed\n",
    "        if epoch > 20:\n",
    "             # Append the logs, losses and accuracies to the lists\n",
    "            self.logs.append(logs)        \n",
    "            self.auc.append(logs.get('auc'))        \n",
    "            self.val_auc.append(logs.get('val_auc'))\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.val_losses.append(logs.get('val_loss'))\n",
    "            best_i = np.argmax(self.val_auc)\n",
    "            grafica_entrenamiento(self.auc, self.val_auc, self.losses, self.val_losses, best_i)\n",
    "\n",
    "plot_losses = TrainingPlot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heatmap(model, im, es_maligna, predictions):\n",
    "    \n",
    "\n",
    "\n",
    "    imag = np.reshape(im, (1, im.shape[0], im.shape[1], im.shape[2]))\n",
    "        \n",
    "    # This is the \"benign\" entry in the prediction vector\n",
    "    output = model.output[0, 0]\n",
    "    \n",
    "    # The is the output feature map of the last convolutional layer\n",
    "    last_conv_layer = model.get_layer('bn')\n",
    "    \n",
    "    # This is the gradient of the \"benign\" class with regard to\n",
    "    # the output feature map of last convolutional layer\n",
    "    grads = K.gradients(output, last_conv_layer.output)[0]\n",
    "    \n",
    "    \n",
    "    # This function allows us to access the values of the quantities we just defined:\n",
    "    # `pooled_grads` and the output feature map of the last convolutional layer\n",
    "    # given a sample image\n",
    "    iterate = K.function([model.input], [last_conv_layer.output, grads])\n",
    "    \n",
    "    # These are the values of these two quantities, as Numpy arrays,\n",
    "    # given our sample image\n",
    "    output, grads_val = iterate(imag)\n",
    "    conv_layer_output_value, pooled_grads_value = output[0, :], grads_val[0, :, :, :]   \n",
    "    \n",
    "   \n",
    "    \n",
    "      \n",
    "    # The channel-wise mean of the resulting feature map\n",
    "    # is our heatmap of class activation\n",
    "    weights = np.mean(pooled_grads_value, axis=(0, 1))\n",
    "    cam = np.dot(conv_layer_output_value, weights)\n",
    "    heatmap = np.maximum(cam, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "    plt.matshow(heatmap)\n",
    "    plt.show()\n",
    "    \n",
    "    # load the original image\n",
    "    img = imag[0]\n",
    "    \n",
    "    # Process CAM\n",
    "    cam = cv2.resize(cam, (img.shape[1], img.shape[0]), cv2.INTER_LINEAR)\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / cam.max()  \n",
    "\n",
    "\n",
    "    \n",
    "    # We resize the heatmap to have the same size as the original image\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    \n",
    "    # We convert the heatmap to RGB\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    \n",
    "    # We apply the heatmap to the original image\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    \n",
    "    superimposed_img = heatmap * 0.8 / 255 + 0.8*img\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(img, vmin=0, vmax=1)\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(heatmap, vmin=0, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(img,\n",
    "                       cmap='gray')\n",
    "    plt.imshow(cam, cmap='jet', alpha=min(0.5, predictions[0]))\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    print(\"- Probabilidad de Exitus:\", predictions[0])\n",
    "    print(\"-\", \"Clase real:\", \"No sobrevive\" if es_maligna else \"Sobrevive\")\n",
    "    print(\"\\n\\n\\n\")\n",
    "    return heatmap, superimposed_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientid</th>\n",
       "      <th>filename</th>\n",
       "      <th>index</th>\n",
       "      <th>SpecificCharacterSet</th>\n",
       "      <th>SOPClassUID</th>\n",
       "      <th>SOPInstanceUID</th>\n",
       "      <th>StudyDate</th>\n",
       "      <th>StudyTime</th>\n",
       "      <th>AccessionNumber</th>\n",
       "      <th>Modality</th>\n",
       "      <th>...</th>\n",
       "      <th>BitsStored</th>\n",
       "      <th>HighBit</th>\n",
       "      <th>PixelRepresentation</th>\n",
       "      <th>LossyImageCompression</th>\n",
       "      <th>LossyImageCompressionMethod</th>\n",
       "      <th>PixelData</th>\n",
       "      <th>image_date</th>\n",
       "      <th>survival</th>\n",
       "      <th>fec_ing</th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>183271.png</td>\n",
       "      <td>one</td>\n",
       "      <td>ISO_IR 100</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.12.2.1107.5.3.56.2693.11.202003240757170156</td>\n",
       "      <td>20200324</td>\n",
       "      <td>75717.0</td>\n",
       "      <td>ACC00005178</td>\n",
       "      <td>CR</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 5987418 elements</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>Y</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>183272.png</td>\n",
       "      <td>one</td>\n",
       "      <td>ISO_IR 100</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.12.2.1107.5.3.56.2693.11.202003240804560406</td>\n",
       "      <td>20200324</td>\n",
       "      <td>75717.0</td>\n",
       "      <td>ACC00005178</td>\n",
       "      <td>CR</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 5877340 elements</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>Y</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>330384.png</td>\n",
       "      <td>one</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.51.0.7.14038266962.53702.43073.38236.31465...</td>\n",
       "      <td>20200327</td>\n",
       "      <td>73607.0</td>\n",
       "      <td>ACC00006084</td>\n",
       "      <td>CR</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 7269056 elements</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>Y</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>480742.png</td>\n",
       "      <td>one</td>\n",
       "      <td>ISO_IR 100</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.12.2.1107.5.3.56.2693.11.202003230325420000</td>\n",
       "      <td>20200323</td>\n",
       "      <td>32542.0</td>\n",
       "      <td>ACC00005564</td>\n",
       "      <td>CR</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 5605756 elements</td>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>Y</td>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>490931.png</td>\n",
       "      <td>one</td>\n",
       "      <td>ISO_IR 100</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.12.2.1107.5.3.56.2693.11.202003190646480968</td>\n",
       "      <td>20200319</td>\n",
       "      <td>64648.0</td>\n",
       "      <td>ACC00004612</td>\n",
       "      <td>CR</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 6076912 elements</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>Y</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>2257</td>\n",
       "      <td>451177.png</td>\n",
       "      <td>one</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.51.0.7.1865235605.2972.30027.36001.12312.3...</td>\n",
       "      <td>20200408</td>\n",
       "      <td>190450.0</td>\n",
       "      <td>ACC00009861</td>\n",
       "      <td>CR</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 4741754 elements</td>\n",
       "      <td>2020-04-08</td>\n",
       "      <td>N</td>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>2269</td>\n",
       "      <td>457682.png</td>\n",
       "      <td>one</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.51.0.7.13791659901.14351.1090.48094.31869....</td>\n",
       "      <td>20200414</td>\n",
       "      <td>171700.0</td>\n",
       "      <td>ACC00009957</td>\n",
       "      <td>CR</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 5894224 elements</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>Y</td>\n",
       "      <td>2020-04-10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>2276</td>\n",
       "      <td>459945.png</td>\n",
       "      <td>one</td>\n",
       "      <td>ISO_IR 100</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.51.0.7.3052672552.30103.8777.44918.7380.44...</td>\n",
       "      <td>20200413</td>\n",
       "      <td>100546.0</td>\n",
       "      <td>ACC00010030</td>\n",
       "      <td>CR</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 6833352 elements</td>\n",
       "      <td>2020-04-13</td>\n",
       "      <td>Y</td>\n",
       "      <td>2020-04-10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>2280</td>\n",
       "      <td>461518.png</td>\n",
       "      <td>one</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.51.0.7.11525005549.51456.28486.35726.61854...</td>\n",
       "      <td>20200415</td>\n",
       "      <td>103742.0</td>\n",
       "      <td>ACC00010093</td>\n",
       "      <td>CR</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 5694582 elements</td>\n",
       "      <td>2020-04-15</td>\n",
       "      <td>Y</td>\n",
       "      <td>2020-04-11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>2287</td>\n",
       "      <td>466281.png</td>\n",
       "      <td>one</td>\n",
       "      <td>ISO_IR 100</td>\n",
       "      <td>Computed Radiography Image Storage</td>\n",
       "      <td>1.3.51.0.7.12387810049.31749.41031.46011.30246...</td>\n",
       "      <td>20200415</td>\n",
       "      <td>95222.0</td>\n",
       "      <td>ACC00009454</td>\n",
       "      <td>CR</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Array of 19441046 elements</td>\n",
       "      <td>2020-04-15</td>\n",
       "      <td>Y</td>\n",
       "      <td>2020-04-11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1278 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      patientid    filename index SpecificCharacterSet  \\\n",
       "0            14  183271.png   one           ISO_IR 100   \n",
       "1            14  183272.png   one           ISO_IR 100   \n",
       "2            18  330384.png   one                  NaN   \n",
       "3            24  480742.png   one           ISO_IR 100   \n",
       "4            25  490931.png   one           ISO_IR 100   \n",
       "...         ...         ...   ...                  ...   \n",
       "1273       2257  451177.png   one                  NaN   \n",
       "1274       2269  457682.png   one                  NaN   \n",
       "1275       2276  459945.png   one           ISO_IR 100   \n",
       "1276       2280  461518.png   one                  NaN   \n",
       "1277       2287  466281.png   one           ISO_IR 100   \n",
       "\n",
       "                             SOPClassUID  \\\n",
       "0     Computed Radiography Image Storage   \n",
       "1     Computed Radiography Image Storage   \n",
       "2     Computed Radiography Image Storage   \n",
       "3     Computed Radiography Image Storage   \n",
       "4     Computed Radiography Image Storage   \n",
       "...                                  ...   \n",
       "1273  Computed Radiography Image Storage   \n",
       "1274  Computed Radiography Image Storage   \n",
       "1275  Computed Radiography Image Storage   \n",
       "1276  Computed Radiography Image Storage   \n",
       "1277  Computed Radiography Image Storage   \n",
       "\n",
       "                                         SOPInstanceUID  StudyDate  StudyTime  \\\n",
       "0       1.3.12.2.1107.5.3.56.2693.11.202003240757170156   20200324    75717.0   \n",
       "1       1.3.12.2.1107.5.3.56.2693.11.202003240804560406   20200324    75717.0   \n",
       "2     1.3.51.0.7.14038266962.53702.43073.38236.31465...   20200327    73607.0   \n",
       "3       1.3.12.2.1107.5.3.56.2693.11.202003230325420000   20200323    32542.0   \n",
       "4       1.3.12.2.1107.5.3.56.2693.11.202003190646480968   20200319    64648.0   \n",
       "...                                                 ...        ...        ...   \n",
       "1273  1.3.51.0.7.1865235605.2972.30027.36001.12312.3...   20200408   190450.0   \n",
       "1274  1.3.51.0.7.13791659901.14351.1090.48094.31869....   20200414   171700.0   \n",
       "1275  1.3.51.0.7.3052672552.30103.8777.44918.7380.44...   20200413   100546.0   \n",
       "1276  1.3.51.0.7.11525005549.51456.28486.35726.61854...   20200415   103742.0   \n",
       "1277  1.3.51.0.7.12387810049.31749.41031.46011.30246...   20200415    95222.0   \n",
       "\n",
       "     AccessionNumber Modality  ...  BitsStored HighBit PixelRepresentation  \\\n",
       "0        ACC00005178       CR  ...          12      11                   0   \n",
       "1        ACC00005178       CR  ...          12      11                   0   \n",
       "2        ACC00006084       CR  ...          12      11                   0   \n",
       "3        ACC00005564       CR  ...          12      11                   0   \n",
       "4        ACC00004612       CR  ...          12      11                   0   \n",
       "...              ...      ...  ...         ...     ...                 ...   \n",
       "1273     ACC00009861       CR  ...          12      11                   0   \n",
       "1274     ACC00009957       CR  ...          12      11                   0   \n",
       "1275     ACC00010030       CR  ...          12      11                   0   \n",
       "1276     ACC00010093       CR  ...          12      11                   0   \n",
       "1277     ACC00009454       CR  ...          15      14                   0   \n",
       "\n",
       "     LossyImageCompression  LossyImageCompressionMethod  \\\n",
       "0                        0                          NaN   \n",
       "1                        0                          NaN   \n",
       "2                        0                          NaN   \n",
       "3                        0                          NaN   \n",
       "4                        0                          NaN   \n",
       "...                    ...                          ...   \n",
       "1273                     0                          NaN   \n",
       "1274                     0                          NaN   \n",
       "1275                     0                          NaN   \n",
       "1276                     0                          NaN   \n",
       "1277                     0                          NaN   \n",
       "\n",
       "                       PixelData  image_date survival     fec_ing days  \n",
       "0      Array of 5987418 elements  2020-03-24        Y  2020-03-21    3  \n",
       "1      Array of 5877340 elements  2020-03-24        Y  2020-03-21    3  \n",
       "2      Array of 7269056 elements  2020-03-27        Y  2020-03-24    3  \n",
       "3      Array of 5605756 elements  2020-03-23        Y  2020-03-20    3  \n",
       "4      Array of 6076912 elements  2020-03-19        Y  2020-03-17    2  \n",
       "...                          ...         ...      ...         ...  ...  \n",
       "1273   Array of 4741754 elements  2020-04-08        N  2020-04-06    2  \n",
       "1274   Array of 5894224 elements  2020-04-14        Y  2020-04-10    4  \n",
       "1275   Array of 6833352 elements  2020-04-13        Y  2020-04-10    3  \n",
       "1276   Array of 5694582 elements  2020-04-15        Y  2020-04-11    4  \n",
       "1277  Array of 19441046 elements  2020-04-15        Y  2020-04-11    4  \n",
       "\n",
       "[1278 rows x 41 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_features = pd.read_csv(os.path.join(path_images, 'clean_data_1_filtered_2.csv'), sep=';');\n",
    "case_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_class = 'survival'\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# ### Train and val\n",
    "# # Split patient ids\n",
    "# positive = case_features[case_features.survival.values == 'Y']\n",
    "# patient_ids = positive.patientid.unique()\n",
    "# original_length = len(patient_ids)\n",
    "# train_ids = np.random.choice(patient_ids, size = math.floor(perc_split[0]*original_length), replace = False)\n",
    "# patient_ids = patient_ids[~np.isin(patient_ids, train_ids)];\n",
    "# val_ids = np.random.choice(patient_ids, size = math.floor(perc_split[1]*original_length), replace = False)\n",
    "# test_ids = patient_ids[~np.isin(patient_ids, val_ids)];\n",
    "\n",
    "# negative = case_features[case_features.survival.values == 'N']\n",
    "# patient_ids = negative.patientid.unique()\n",
    "# original_length = len(patient_ids)\n",
    "# train_ids = np.append(train_ids, np.random.choice(patient_ids, size = math.floor(perc_split[0]*original_length), replace = False))\n",
    "# patient_ids = patient_ids[~np.isin(patient_ids, train_ids)];\n",
    "# val_ids = np.append(val_ids, np.random.choice(patient_ids, size = math.floor(perc_split[1]*original_length), replace = False))\n",
    "# test_ids = np.append(test_ids, patient_ids[~np.isin(patient_ids, val_ids)]);\n",
    "\n",
    "# # Split dataset based on patient ids\n",
    "# case_features_train = case_features[case_features.patientid.isin(train_ids)]\n",
    "# case_features_val = case_features[case_features.patientid.isin(val_ids)]\n",
    "# case_features_test = case_features[case_features.patientid.isin(test_ids)]\n",
    "\n",
    "# # Selección del patrón de datos X y del target y\n",
    "# ytrain = case_features_train[target_class]\n",
    "# del case_features_train[target_class]\n",
    "# Xtrain = case_features_train\n",
    "\n",
    "# yval = case_features_val[target_class]\n",
    "# del case_features_val[target_class]\n",
    "# Xval = case_features_val\n",
    "\n",
    "# ytest = case_features_test[target_class]\n",
    "# del case_features_test[target_class]\n",
    "# Xtest = case_features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(case_features_train[ytrain == \"Y\"].patientid.unique()))\n",
    "# print(len(case_features_train[ytrain == \"N\"].patientid.unique()))\n",
    "# print(len(case_features_val[yval == \"Y\"].patientid.unique()))\n",
    "# print(len(case_features_val[yval == \"N\"].patientid.unique()))\n",
    "# print(len(case_features_test[ytest == \"Y\"].patientid.unique()))\n",
    "# print(len(case_features_test[ytest == \"N\"].patientid.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(case_features.shape)\n",
    "# print(case_features_train.shape)\n",
    "# print(case_features_val.shape)\n",
    "# print(case_features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ytrain.value_counts())\n",
    "# print(yval.value_counts())\n",
    "# print(ytest.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = []\n",
    "# for i in range(Xtrain.shape[0]):\n",
    "#     # print('%0.2f%%' % float(100*i/Xtrain.shape[0]))\n",
    "#     image_path = os.path.join(path_images, 'processed', Xtrain.iloc[i].filename)\n",
    "#     imagen = Image.open(image_path)\n",
    "#     imagen = np.asarray(imagen.convert(\"RGB\"))\n",
    "#     imagen = resize(imagen,  input_shape)\n",
    "#     X_train.append(imagen)\n",
    "\n",
    "# X_train = np.stack(X_train, axis = 0)\n",
    "# np.save(os.path.join(path_images, 'X_train.npy'), X_train)\n",
    "# print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val = []\n",
    "# for i in range(Xval.shape[0]):\n",
    "#     # print('%0.2f%%' % float(100*i/Xval.shape[0]))\n",
    "#     image_path = os.path.join(path_images, 'processed', Xval.iloc[i].filename)\n",
    "#     imagen = Image.open(image_path)\n",
    "#     imagen = np.asarray(imagen.convert(\"RGB\"))\n",
    "#     imagen = resize(imagen,  input_shape)\n",
    "#     X_val.append(imagen)\n",
    "\n",
    "# X_val = np.stack(X_val, axis = 0)\n",
    "# np.save(os.path.join(path_images, 'X_val.npy'), X_val)\n",
    "# print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = []\n",
    "# for i in range(Xtest.shape[0]):\n",
    "#     # print('%0.2f%%' % float(100*i/Xtest.shape[0]))\n",
    "#     image_path = os.path.join(path_images, 'processed', Xtest.iloc[i].filename)\n",
    "#     imagen = Image.open(image_path)\n",
    "#     imagen = np.asarray(imagen.convert(\"RGB\"))\n",
    "#     imagen = resize(imagen,  input_shape)\n",
    "#     X_test.append(imagen)\n",
    "\n",
    "# X_test = np.stack(X_test, axis = 0)\n",
    "# np.save(os.path.join(path_images, 'X_test.npy'), X_test)\n",
    "# print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ytrain[ytrain=='Y'] = 0\n",
    "# ytrain[ytrain=='N'] = 1\n",
    "# y_train = np.array(ytrain, dtype = np.int64)\n",
    "# y_train.shape\n",
    "# np.save(os.path.join(path_images, 'y_train.npy'), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yval[yval=='Y'] = 0\n",
    "# yval[yval=='N'] = 1\n",
    "# y_val = np.array(yval, dtype = np.int64)\n",
    "# y_val.shape\n",
    "# np.save(os.path.join(path_images, 'y_val.npy'), y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ytest[ytest=='Y'] = 0\n",
    "# ytest[ytest=='N'] = 1\n",
    "# y_test = np.array(ytest, dtype = np.int64)\n",
    "# y_test.shape\n",
    "# np.save(os.path.join(path_images, 'y_test.npy'), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(os.path.join(path_images, 'X_train.npy'))\n",
    "X_val = np.load(os.path.join(path_images, 'X_val.npy'))\n",
    "X_test = np.load(os.path.join(path_images, 'X_test.npy'))\n",
    "y_train = np.load(os.path.join(path_images, 'y_train.npy'))\n",
    "y_val = np.load(os.path.join(path_images, 'y_val.npy'))\n",
    "y_test = np.load(os.path.join(path_images, 'y_test.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84575835 0.15424165]\n",
      "[0.85772358 0.14227642]\n",
      "[0.8503937 0.1496063]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "ratio_pos = np.count_nonzero(y_train == 0) / len(y_train)\n",
    "ratio_neg = np.count_nonzero(y_train == 1) / len(y_train)\n",
    "class_weight_train = np.array((ratio_pos, ratio_neg))\n",
    "\n",
    "# val\n",
    "ratio_pos = np.count_nonzero(y_val == 0) / len(y_val)\n",
    "ratio_neg = np.count_nonzero(y_val == 1) / len(y_val)\n",
    "class_weight_val = np.array((ratio_pos, ratio_neg))\n",
    "\n",
    "# test\n",
    "ratio_pos = np.count_nonzero(y_test == 0) / len(y_test)\n",
    "ratio_neg = np.count_nonzero(y_test == 1) / len(y_test)\n",
    "class_weight_test= np.array((ratio_pos, ratio_neg))\n",
    "\n",
    "# Print\n",
    "print(class_weight_train)\n",
    "print(class_weight_val)\n",
    "print(class_weight_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=True, \n",
    "                             featurewise_std_normalization=True,\n",
    "                             brightness_range = (0.25, 0.75))\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(\n",
    "    os.getcwd(),\n",
    "    '../saved_models'\n",
    ")\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# This callback saves the weights of the model after each epoch\n",
    "checkpoint = ModelCheckpoint(\n",
    "    '../saved_models/weights.epoch_{epoch:02d}.hdf5',\n",
    "    monitor='val_loss', \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# This callback writes logs for TensorBoard\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir='./Graph', \n",
    "    histogram_freq=0,  \n",
    "    write_graph=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_auc', patience=400, verbose=1, mode='max', restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=100, mode='min')\n",
    "\n",
    "\n",
    "\n",
    "callbacks_list = [early_stopping]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "### Hyperparameters\n",
    "batch_size_values = [32, 64, 128, 512]\n",
    "learning_rate_values = [0.0001, 0.001, 0.01, 1]\n",
    "neurons_values = [64, 128, 256, 512]\n",
    "l2_values = [0.001, 0.01, 0.1, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of batch_size = 32 - learning_rate = 0.000100 - neurons = 64 - l2 = 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda/python-3.6/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00574: early stopping\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00564: early stopping\n",
      "Start of batch_size = 32 - learning_rate = 0.000100 - neurons = 64 - l2 = 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda/python-3.6/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00450: early stopping\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00529: early stopping\n",
      "Start of batch_size = 32 - learning_rate = 0.000100 - neurons = 64 - l2 = 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda/python-3.6/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00434: early stopping\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00493: early stopping\n",
      "Start of batch_size = 32 - learning_rate = 0.000100 - neurons = 64 - l2 = 0.500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda/python-3.6/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00467: early stopping\n"
     ]
    }
   ],
   "source": [
    "grid_results = pd.DataFrame()\n",
    "for batch_size in batch_size_values:\n",
    "    for learning_rate in learning_rate_values:\n",
    "        for neurons in neurons_values:\n",
    "            for l2 in l2_values:\n",
    "                \n",
    "                ### Print log\n",
    "                print (\"Start of batch_size = %d - learning_rate = %f - neurons = %d - l2 = %f\" % \n",
    "                       (batch_size, learning_rate, neurons, l2))\n",
    "                \n",
    "                ### Standard model\n",
    "                \n",
    "                # Create model\n",
    "                base_model, model = get_model(learning_rate, neurons, l2)\n",
    "                \n",
    "                # Train\n",
    "                history = model.fit(X_train,y_train,\n",
    "                          validation_data=(X_val, y_val),\n",
    "                          batch_size=batch_size, \n",
    "                          nb_epoch=epochs,\n",
    "                          class_weight=class_weight_train,\n",
    "                          callbacks = callbacks_list,\n",
    "                          verbose=0)\n",
    "                \n",
    "                # Predict\n",
    "                pred_train = model.predict(X_train)\n",
    "                pred_val = model.predict(X_val)\n",
    "                pred_test = model.predict(X_test)\n",
    "                \n",
    "                # Get metrics\n",
    "                stopped_epoch = early_stopping.stopped_epoch\n",
    "                auc_train = roc_auc_score(y_true = y_train, y_score = pred_train)\n",
    "                auc_val = roc_auc_score(y_true = y_val, y_score = pred_val)\n",
    "                auc_test = roc_auc_score(y_true = y_test, y_score = pred_test)\n",
    "                \n",
    "                # Save results\n",
    "                res = pd.DataFrame([batch_size, learning_rate, neurons, l2, \n",
    "                                    stopped_epoch, auc_train, auc_val, auc_test])\n",
    "                grid_results = pd.concat([grid_results, res], axis=1)\n",
    "                \n",
    "                # Free memory                \n",
    "                del base_model, model\n",
    "                \n",
    "                \n",
    "                ### Augmented model\n",
    "                \n",
    "                # Create model\n",
    "                base_model_augmented, model_augmented = get_model(learning_rate, neurons, l2)\n",
    "                \n",
    "                # Train\n",
    "                history = model_augmented.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size, seed=seed),\n",
    "                     validation_data=(X_val, y_val),\n",
    "                     steps_per_epoch=len(X_train) / batch_size, \n",
    "                     epochs=epochs,\n",
    "                     class_weight=class_weight_train,\n",
    "                     callbacks = callbacks_list,       \n",
    "                     verbose=0)\n",
    "                \n",
    "                # Predict\n",
    "                pred_train = model_augmented.predict(X_train)\n",
    "                pred_val = model_augmented.predict(X_val)\n",
    "                pred_test = model_augmented.predict(X_test)\n",
    "                \n",
    "                # Get metrics\n",
    "                stopped_epoch = early_stopping.stopped_epoch\n",
    "                auc_train = roc_auc_score(y_true = y_train, y_score = pred_train)\n",
    "                auc_val = roc_auc_score(y_true = y_val, y_score = pred_val)\n",
    "                auc_test = roc_auc_score(y_true = y_test, y_score = pred_test)\n",
    "                \n",
    "                # Save results\n",
    "                res = pd.DataFrame([batch_size, learning_rate, neurons, l2, \n",
    "                                    stopped_epoch, auc_train, auc_val, auc_test])\n",
    "                grid_results = pd.concat([grid_results, res], axis=1)      \n",
    "                \n",
    "                # Free memory   \n",
    "                del base_model_augmented, model_augmented\n",
    "                \n",
    "                \n",
    "                # Save intermediate results\n",
    "                inner_res = grid_results.copy()\n",
    "                inner_res.index = ['batch_size', 'learning_rate', 'neurons', 'l2', \n",
    "                                      'stopped_epoch', 'auc_train', 'auc_val', 'auc_test']\n",
    "                inner_res.T.to_csv(os.path.join(path_results,'grid_results.csv')) \n",
    "\n",
    "# Save final results\n",
    "grid_results.index = ['batch_size', 'learning_rate', 'neurons', 'l2', \n",
    "                      'stopped_epoch', 'auc_train', 'auc_val', 'auc_test']\n",
    "grid_results.T.to_csv(os.path.join(path_results,'grid_results.csv'))      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
