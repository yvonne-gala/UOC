{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFss6dNFTSmn"
   },
   "source": [
    "# Convolutional AE con Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nPYuKtjYTSD_"
   },
   "outputs": [],
   "source": [
    "# Generic\n",
    "# numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Generic\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Images\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "# import talos as ta\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, plot_confusion_matrix, roc_curve\n",
    "\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "\n",
    "# Keras\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.utils import print_summary\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.applications.densenet import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_images = '/home/jesusprada/proyecto_python/x-ray/data/covid_images'\n",
    "path_data = '/home/jesusprada/proyecto_python/x-ray/data'\n",
    "path_results = '/home/jesusprada/proyecto_python/x-ray/results_mbit'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Classes and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CheXnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chexNet weights\n",
    "chexnet_weights = '/home/ygala/TFM_UOC/scripts/chexnet/best_weights.h5'\n",
    "\n",
    "def chexnet_preprocess_input(value):\n",
    "    return preprocess_input(value)\n",
    "\n",
    "\n",
    "def get_chexnet_model():\n",
    "    input_shape = (224, 224, 3)\n",
    "    img_input = Input(shape=input_shape)\n",
    "    base_weights = 'imagenet'\n",
    "\n",
    "    # create the base pre-trained model\n",
    "    base_model = DenseNet121(\n",
    "        include_top=False,\n",
    "        input_tensor=img_input,\n",
    "        input_shape=input_shape,\n",
    "        weights=base_weights,\n",
    "        pooling='avg'\n",
    "    )\n",
    "\n",
    "    x = base_model.output\n",
    "    # add a logistic layer -- let's say we have 14 classes\n",
    "    predictions = Dense(\n",
    "        14,\n",
    "        activation='sigmoid',\n",
    "        name='predictions')(x)\n",
    "\n",
    "    # this is the model we will use\n",
    "    model = Model(\n",
    "        inputs=img_input,\n",
    "        outputs=predictions,\n",
    "    )\n",
    "\n",
    "    # load chexnet weights\n",
    "    model.load_weights(chexnet_weights)\n",
    "\n",
    "    # return model\n",
    "    return base_model, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_class_weight(csv_file_path, target_class):\n",
    "    df = pd.read_csv(csv_file_path, sep=';')\n",
    "    total_counts = df.shape[0]\n",
    "    class_weight = []\n",
    "\n",
    "    ratio_pos = df.loc[(df[target_class] == 'Y')].shape[0] / total_counts\n",
    "    ratio_neg = df.loc[(df[target_class] == 'N')].shape[0] / total_counts\n",
    "    class_weight = np.array((ratio_pos, ratio_neg))\n",
    "        \n",
    "    return class_weight\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "\n",
    "def print_confidence_intervals(statistics):\n",
    "    df = pd.DataFrame(columns=[\"Mean AUC (CI 5%-95%)\"])\n",
    "    mean = statistics.mean()\n",
    "    max_ = np.quantile(statistics, .95)\n",
    "    min_ = np.quantile(statistics, .05)\n",
    "    df.loc[\"Exitus\"] = [\"%.2f (%.2f-%.2f)\" % (mean, min_, max_)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_model(learning_rate):\n",
    "    # get base model, model\n",
    "    base_model, chexnet_model = get_chexnet_model()\n",
    "\n",
    "    x = base_model.output\n",
    "    \n",
    "    # Regularization layer\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Dense layer\n",
    "    x = Dense(128, \n",
    "              activation='relu',\n",
    "              kernel_regularizer=tf.keras.regularizers.l1_l2(0.5, 0.0001))(x)\n",
    "    \n",
    "    # Regularization layer\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    \n",
    "    # add a logistic layer -- let's say we have 6 classes\n",
    "    predictions = Dense(\n",
    "        1,\n",
    "        activation='sigmoid')(x)\n",
    "\n",
    "    # this is the model we will use\n",
    "    model = Model(\n",
    "        inputs=base_model.input,\n",
    "        outputs=predictions,\n",
    "    )\n",
    "\n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # initiate an Adam optimizer\n",
    "    opt = Adam(\n",
    "        lr=learning_rate,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        decay=0.0,\n",
    "        amsgrad=False\n",
    "    )\n",
    "\n",
    "    # Let's train the model using Adam\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=[metrics.BinaryAccuracy(name = \"acc\"),\n",
    "                metrics.AUC(name = \"auc\")])\n",
    "\n",
    "    return base_model, model\n",
    "\n",
    "\n",
    "class print_learning_rate(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        print(f'Learning rate = {K.eval(lr):.5f}')\n",
    "print_lr = print_learning_rate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grafica_entrenamiento(tr_auc, val_auc, tr_loss, val_loss, best_i,\n",
    "                          figsize=(10,5), path_results = None):\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = plt.subplot(1,2,1)\n",
    "    plt.plot(1+np.arange(len(tr_loss)), np.array(tr_loss))\n",
    "    plt.plot(1+np.arange(len(val_loss)), np.array(val_loss))\n",
    "    plt.plot(1+best_i, val_loss[best_i], 'or')\n",
    "    plt.title('loss del modelo', fontsize=18)\n",
    "    plt.ylabel('loss', fontsize=12)\n",
    "    plt.xlabel('época', fontsize=18)        \n",
    "    plt.legend(['entrenamiento', 'validación'], loc='upper left')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    \n",
    "    plt.plot(1+np.arange(len(tr_auc)),  np.array(tr_auc))\n",
    "    plt.plot(1+np.arange(len(val_auc)), np.array(val_auc))\n",
    "    plt.plot(1+best_i, val_auc[best_i], 'or')\n",
    "    plt.title('AUC', fontsize=18)\n",
    "    plt.ylabel('AUC', fontsize=12)\n",
    "    plt.xlabel('época', fontsize=18)    \n",
    "    plt.legend(['entrenamiento', 'validación'], loc='upper left')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    if (path_results != None):\n",
    "        plt.savefig(os.path.join(path_results, 'auc_loss.png'))\n",
    "    plt.show()\n",
    "    \n",
    "class TrainingPlot(Callback):\n",
    "    \n",
    "    # This function is called when the training begins\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # Initialize the lists for holding the logs, losses and accuracies\n",
    "        self.losses = []\n",
    "        self.auc = []\n",
    "        self.val_losses = []\n",
    "        self.val_auc = []\n",
    "        self.logs = []\n",
    "    \n",
    "    # This function is called at the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        # Append the logs, losses and accuracies to the lists\n",
    "        self.logs.append(logs)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.auc.append(logs.get('auc'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.val_auc.append(logs.get('val_auc'))\n",
    "        \n",
    "        # Before plotting ensure at least 2 epochs have passed\n",
    "        if len(self.val_auc) > 1:\n",
    "            best_i = np.argmax(self.val_auc)\n",
    "            grafica_entrenamiento(self.auc, self.val_auc, self.losses, self.val_losses, best_i)\n",
    "\n",
    "plot_losses = TrainingPlot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function for painting heatmaps\n",
    "\n",
    "def show_heatmap(model, im, es_maligna, predictions):\n",
    "\n",
    "    imag = np.reshape(im, (1, im.shape[0], im.shape[1], im.shape[2]))\n",
    "        \n",
    "    # This is the \"benign\" entry in the prediction vector\n",
    "    output = model.output[0, 0]\n",
    "    \n",
    "    # The is the output feature map of the last convolutional layer\n",
    "    last_conv_layer = model.get_layer('bn')\n",
    "    \n",
    "    # This is the gradient of the \"benign\" class with regard to\n",
    "    # the output feature map of last convolutional layer\n",
    "    grads = K.gradients(output, last_conv_layer.output)[0]\n",
    "    \n",
    "    \n",
    "    # This function allows us to access the values of the quantities we just defined:\n",
    "    iterate = K.function([model.input], [last_conv_layer.output, grads])\n",
    "    \n",
    "    # These are the values of these two quantities, as Numpy arrays,\n",
    "    # given our sample image\n",
    "    output, grads_val = iterate(imag)\n",
    "    conv_layer_output_value, pooled_grads_value = output[0, :], grads_val[0, :, :, :]   \n",
    "    \n",
    "   \n",
    "    \n",
    "      \n",
    "    # The channel-wise mean of the resulting feature map\n",
    "    # is our heatmap of class activation\n",
    "    weights = np.mean(pooled_grads_value, axis=(0, 1))\n",
    "    cam = np.dot(conv_layer_output_value, weights)\n",
    "    heatmap = np.maximum(cam, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "    plt.matshow(heatmap)\n",
    "    plt.show()\n",
    "    \n",
    "    # load the original image\n",
    "    img = imag[0]\n",
    "    \n",
    "    # Process CAM\n",
    "    cam = cv2.resize(cam, (img.shape[1], img.shape[0]), cv2.INTER_LINEAR)\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / cam.max()  \n",
    "\n",
    "\n",
    "    \n",
    "    # We resize the heatmap to have the same size as the original image\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    \n",
    "    # We convert the heatmap to RGB\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    \n",
    "    # We apply the heatmap to the original image\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    \n",
    "    superimposed_img = heatmap * 0.8 / 255 + 0.8*img\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(img, vmin=0, vmax=1)\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(heatmap, vmin=0, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(img,\n",
    "                       cmap='gray')\n",
    "    plt.imshow(cam, cmap='jet', alpha=min(0.5, predictions[0]))\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    print(\"- Probabilidad de Exitus:\", predictions[0])\n",
    "    print(\"-\", \"Clase real:\", \"No sobrevive\" if es_maligna else \"Sobrevive\")\n",
    "    print(\"\\n\\n\\n\")\n",
    "    return heatmap, superimposed_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(os.path.join(path_data, 'X_train_filter.npy'))\n",
    "X_val = np.load(os.path.join(path_data, 'X_val_filter.npy'))\n",
    "X_test = np.load(os.path.join(path_data, 'X_test_filter.npy'))\n",
    "\n",
    "ytrain = np.load(os.path.join(path_data, 'ytrain_filter.npy'), allow_pickle=True)\n",
    "yval = np.load(os.path.join(path_data, 'yval_filter.npy'), allow_pickle=True)\n",
    "ytest = np.load(os.path.join(path_data, 'ytest_filter.npy'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split patient data death and survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = X_train[ytrain==1]\n",
    "X_train_0 = X_train[ytrain==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set class weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_pos = np.count_nonzero(y_train == 0) / len(y_train)\n",
    "ratio_neg = np.count_nonzero(y_train == 1) / len(y_train)\n",
    "class_weight_train = np.array((ratio_pos, ratio_neg))\n",
    "print(class_weight_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_pos = np.count_nonzero(y_val == 0) / len(y_val)\n",
    "ratio_neg = np.count_nonzero(y_val == 1) / len(y_val)\n",
    "class_weight_val = np.array((ratio_pos, ratio_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_pos = np.count_nonzero(y_test == 0) / len(y_test)\n",
    "ratio_neg = np.count_nonzero(y_test == 1) / len(y_test)\n",
    "class_weight_test = np.array((ratio_pos, ratio_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uSq5lDFkWAh_"
   },
   "outputs": [],
   "source": [
    "# MODELO\n",
    "\n",
    "# ENCODER\n",
    "input_img = Input(shape=(28, 28, 1))  \n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# DECODER\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xIrlLun-TFA4"
   },
   "outputs": [],
   "source": [
    "# DATOS\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1)) \n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1853
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 329168,
     "status": "ok",
     "timestamp": 1553094845162,
     "user": {
      "displayName": "Valero Laparra",
      "photoUrl": "",
      "userId": "00355299981903664579"
     },
     "user_tz": -60
    },
    "id": "3defT4BpTG5r",
    "outputId": "2fcc7773-bd68-41bf-de83-0ab2c5f69e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.2213 - val_loss: 0.1660\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.1564 - val_loss: 0.1422\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.1406 - val_loss: 0.1397\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1323 - val_loss: 0.1280\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1273 - val_loss: 0.1248\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.1237 - val_loss: 0.1196\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.1214 - val_loss: 0.1236\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1194 - val_loss: 0.1184\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1182 - val_loss: 0.1174\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1171 - val_loss: 0.1170\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1162 - val_loss: 0.1177\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1153 - val_loss: 0.1104\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.1141 - val_loss: 0.1121\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.1133 - val_loss: 0.1138\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1125 - val_loss: 0.1121\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1116 - val_loss: 0.1119\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1109 - val_loss: 0.1122\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1103 - val_loss: 0.1101\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1100 - val_loss: 0.1092\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1098 - val_loss: 0.1089\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1092 - val_loss: 0.1059\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1086 - val_loss: 0.1076\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1086 - val_loss: 0.1082\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1083 - val_loss: 0.1084\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.1077 - val_loss: 0.1075\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.1073 - val_loss: 0.1076\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.1069 - val_loss: 0.1060\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1069 - val_loss: 0.1100\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1062 - val_loss: 0.1062\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1064 - val_loss: 0.1056\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1063 - val_loss: 0.1063\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.1061 - val_loss: 0.1047\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1057 - val_loss: 0.1074\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1055 - val_loss: 0.1030\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1052 - val_loss: 0.1041\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1051 - val_loss: 0.1057\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1047 - val_loss: 0.1025\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1044 - val_loss: 0.1040\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1043 - val_loss: 0.1063\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1039 - val_loss: 0.1058\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1038 - val_loss: 0.1048\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.1037 - val_loss: 0.1016\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1037 - val_loss: 0.1013\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1034 - val_loss: 0.1030\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1034 - val_loss: 0.1038\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.1031 - val_loss: 0.1007\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1030 - val_loss: 0.1029\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1028 - val_loss: 0.1018\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1027 - val_loss: 0.1029\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 0.1027 - val_loss: 0.1004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6bea1e7ef0>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ENTRENAMIENTO\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TYGhJutTIm8"
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "decoded_imgs = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 330396,
     "status": "ok",
     "timestamp": 1553094846424,
     "user": {
      "displayName": "Valero Laparra",
      "photoUrl": "",
      "userId": "00355299981903664579"
     },
     "user_tz": -60
    },
    "id": "h3Kzz_hYTKcG",
    "outputId": "85a1fa69-fe84-4c2b-e769-05b653bb060d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  \n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruccion\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)mentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=True, \n",
    "                             featurewise_std_normalization=True, \n",
    "                             rotation_range=90)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CNN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed\n",
    "base_model, model = get_model(learning_rate)\n",
    "\n",
    "# Show layers\n",
    "print_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read created data by GANs\n",
    "\n",
    "import random\n",
    "\n",
    "previous_val_auc = 0\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for n in size_ae:\n",
    "    print (\"Numero imagenes GANs %d\" % n)\n",
    "    \n",
    "    X_train = np.load(os.path.join(path_images, 'X_train.npy'))\n",
    "    X_val = np.load(os.path.join(path_images, 'X_val.npy'))\n",
    "    X_test = np.load(os.path.join(path_images, 'X_test.npy'))\n",
    "    y_train = np.load(os.path.join(path_images, 'y_train.npy'))\n",
    "    y_val = np.load(os.path.join(path_images, 'y_val.npy'))\n",
    "    y_test = np.load(os.path.join(path_images, 'y_test.npy'))\n",
    "    \n",
    "    if n != 0:\n",
    "        X_train_0_ae = np.load(os.path.join(path_ae, 'AE_covid_0_num_imag_%d.npy' % n))\n",
    "        X_train_1_ae = np.load(os.path.join(path_ae, 'AE_covid_1_num_imag_%d.npy'% n))\n",
    "\n",
    "        ratio_neg = np.count_nonzero(y_train == 1) / len(y_train)\n",
    "        size = round(ratio_neg * X_train_1_ae.shape[0])\n",
    "        lista = range(X_train_1_ae.shape[0]-1)\n",
    "\n",
    "\n",
    "        ind = random.sample(lista, size)\n",
    "        X_train_1_ae = X_train_1_ae[ind,:]\n",
    "\n",
    "        # create target\n",
    "        y_train_1_ae = np.ones(X_train_1_ae.shape[0], dtype=int)\n",
    "        y_train_0_ae = np.zeros(X_train_0_ae.shape[0], dtype=int)\n",
    "\n",
    "\n",
    "        # join data\n",
    "\n",
    "        X_train_ae = np.concatenate((X_train_1_ae, X_train_0_ae))\n",
    "        y_train_ae = np.concatenate((y_train_1_ae, y_train_0_ae))\n",
    "\n",
    "        X_train = np.concatenate((X_train, X_train_ae))\n",
    "        y_train = np.concatenate((y_train, y_train_ae))\n",
    "    \n",
    "    \n",
    "    # train\n",
    "    ratio_pos = np.count_nonzero(y_train == 0) / len(y_train)\n",
    "    ratio_neg = np.count_nonzero(y_train == 1) / len(y_train)\n",
    "    class_weight_train = np.array((ratio_pos, ratio_neg))\n",
    "\n",
    "    # val\n",
    "    ratio_pos = np.count_nonzero(y_val == 0) / len(y_val)\n",
    "    ratio_neg = np.count_nonzero(y_val == 1) / len(y_val)\n",
    "    class_weight_val = np.array((ratio_pos, ratio_neg))\n",
    "\n",
    "    \n",
    "    out = model.fit(X_train, y_train,\n",
    "                     validation_data=(X_val, y_val),\n",
    "                     steps_per_epoch=len(X_train) / batch_size, \n",
    "                     epochs=epochs,\n",
    "                     class_weight=class_weight_train,\n",
    "                     callbacks = callbacks_list,       \n",
    "                     verbose=1)\n",
    "    \n",
    "    if(len(out.history)):\n",
    "        acum_tr_auc.append(out.history['auc'][0])\n",
    "        acum_val_auc.append(out.history['val_auc'][0])\n",
    "        acum_tr_loss.append(out.history['loss'][0])\n",
    "        acum_val_loss.append(out.history['val_loss'][0])\n",
    "                \n",
    "        acum = pd.DataFrame([acum_tr_auc, acum_val_auc, acum_tr_loss, acum_val_loss])\n",
    "        acum.to_csv(os.path.join(path_results,'acum_results_no_augmentation.csv'))\n",
    "                    \n",
    "        #if len(acum_tr_auc) > 1:\n",
    "        clear_output()\n",
    "        best_i = np.argmax(acum_val_auc)\n",
    "        grafica_entrenamiento(acum_tr_auc, acum_val_auc, acum_tr_loss, acum_val_loss, best_i)\n",
    "            ### save loss and auc of train and val     \n",
    "        stopped_epoch = early_stopping.stopped_epoch\n",
    "        train_loss = out.history['loss'][stopped_epoch-1]\n",
    "        val_loss = out.history['val_loss'][stopped_epoch-1]\n",
    "        train_auc = out.history['auc'][stopped_epoch-1]\n",
    "        val_auc = out.history['val_auc'][stopped_epoch-1]\n",
    "        model = out.model\n",
    "\n",
    "        pred_train = model.predict(X_train)\n",
    "        pred_val = model.predict(X_val)\n",
    "\n",
    "        train_auc = roc_auc_score(y_true = y_train, y_score = pred_train)\n",
    "        val_auc = roc_auc_score(y_true = y_val, y_score = pred_val)\n",
    "\n",
    "        \n",
    "        res = pd.DataFrame([n, epochs, batch_size, stopped_epoch, train_auc, val_auc])\n",
    "        df = pd.concat([df, res], axis=1)\n",
    "\n",
    "        df.to_csv(os.path.join(path_results,'model_results_augmentation.csv')) \n",
    "                \n",
    "        model.save(os.path.join(path_results, 'model_augmentation_%d.h5' % n))\n",
    "            \n",
    "        if(previous_val_auc < val_auc):\n",
    "            save_dir = os.path.join(\n",
    "                    os.getcwd(),\n",
    "                    '../model_results_y')\n",
    "            if not os.path.isdir(save_dir):\n",
    "                    os.makedirs(save_dir)\n",
    "                model.save(os.path.join(path_results, 'model_no_augmentation_%d.h5' % n))\n",
    "                \n",
    "            previous_val_auc = val_auc\n",
    "                \n",
    "\n",
    "                           \n",
    "df.index = ['size_ae', 'epochs', 'batch_size','early_stopping', 'train_auc', 'val_auc']\n",
    "df.to_csv(os.path.join(path_results,'model_results_augmentation.csv'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read created data by GANs\n",
    "\n",
    "import random\n",
    "\n",
    "previous_val_auc = 0\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for n in size_ae:\n",
    "    print (\"Numero imagenes autoencoders %d\" % n)\n",
    "    \n",
    "    X_train = np.load(os.path.join(path_images, 'X_train.npy'))\n",
    "    X_val = np.load(os.path.join(path_images, 'X_val.npy'))\n",
    "    X_test = np.load(os.path.join(path_images, 'X_test.npy'))\n",
    "    y_train = np.load(os.path.join(path_images, 'y_train.npy'))\n",
    "    y_val = np.load(os.path.join(path_images, 'y_val.npy'))\n",
    "    y_test = np.load(os.path.join(path_images, 'y_test.npy'))\n",
    "    \n",
    "    if n != 0:\n",
    "        X_train_0_ae= np.load(os.path.join(path_ae, 'AE_covid_0_num_imag_%d.npy' % n))\n",
    "        X_train_1_ae = np.load(os.path.join(path_ae, 'AE_covid_1_num_imag_%d.npy'% n))\n",
    "\n",
    "        ratio_neg = np.count_nonzero(y_train == 1) / len(y_train)\n",
    "        size = round(ratio_neg * X_train_1_ae.shape[0])\n",
    "        lista = range(X_train_1_ae.shape[0]-1)\n",
    "\n",
    "\n",
    "        ind = random.sample(lista, size)\n",
    "        X_train_1_ae = X_train_1_ae[ind,:]\n",
    "\n",
    "        # create target\n",
    "        y_train_1_ae = np.ones(X_train_1_ae.shape[0], dtype=int)\n",
    "        y_train_0_ae = np.zeros(X_train_0_ae.shape[0], dtype=int)\n",
    "\n",
    "\n",
    "        # join data\n",
    "\n",
    "        X_train_ae = np.concatenate((X_train_1_ae, X_train_0_ae))\n",
    "        y_train_ae = np.concatenate((y_train_1_ae, y_train_0_ae))\n",
    "\n",
    "        X_train = np.concatenate((X_train, X_train_ae))\n",
    "        y_train = np.concatenate((y_train, y_train_ae))\n",
    "    \n",
    "    \n",
    "    # train\n",
    "    ratio_pos = np.count_nonzero(y_train == 0) / len(y_train)\n",
    "    ratio_neg = np.count_nonzero(y_train == 1) / len(y_train)\n",
    "    class_weight_train = np.array((ratio_pos, ratio_neg))\n",
    "\n",
    "    # val\n",
    "    ratio_pos = np.count_nonzero(y_val == 0) / len(y_val)\n",
    "    ratio_neg = np.count_nonzero(y_val == 1) / len(y_val)\n",
    "    class_weight_val = np.array((ratio_pos, ratio_neg))\n",
    "\n",
    "    datagen = ImageDataGenerator(featurewise_center=True, \n",
    "                             featurewise_std_normalization=True,\n",
    "                             rotation_range=90,\n",
    "                             brightness_range = (0.25, 0.75))\n",
    "    datagen.fit(X_train)\n",
    "    \n",
    "    base_model_augmented, model_augmented = get_model(learning_rate)\n",
    "    \n",
    "    out = model_augmented.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size, seed=seed),\n",
    "                     validation_data=(X_val, y_val),\n",
    "                     steps_per_epoch=len(X_train) / batch_size, \n",
    "                     epochs=epochs,\n",
    "                     class_weight=class_weight_train,\n",
    "                     callbacks = callbacks_list,       \n",
    "                     verbose=1)\n",
    "    \n",
    "    if(len(out.history)):\n",
    "        acum_tr_auc.append(out.history['auc'][0])\n",
    "        acum_val_auc.append(out.history['val_auc'][0])\n",
    "        acum_tr_loss.append(out.history['loss'][0])\n",
    "        acum_val_loss.append(out.history['val_loss'][0])\n",
    "                \n",
    "        acum = pd.DataFrame([acum_tr_auc, acum_val_auc, acum_tr_loss, acum_val_loss])\n",
    "        acum.to_csv(os.path.join(path_results,'acum_results_no_augmentation.csv'))\n",
    "                    \n",
    "        #if len(acum_tr_auc) > 1:\n",
    "        clear_output()\n",
    "        best_i = np.argmax(acum_val_auc)\n",
    "        grafica_entrenamiento(acum_tr_auc, acum_val_auc, acum_tr_loss, acum_val_loss, best_i)\n",
    "            ### save loss and auc of train and val     \n",
    "        stopped_epoch = early_stopping.stopped_epoch\n",
    "        train_loss = out.history['loss'][stopped_epoch-1]\n",
    "        val_loss = out.history['val_loss'][stopped_epoch-1]\n",
    "        train_auc = out.history['auc'][stopped_epoch-1]\n",
    "        val_auc = out.history['val_auc'][stopped_epoch-1]\n",
    "        model = out.model\n",
    "\n",
    "        pred_train = model.predict(X_train)\n",
    "        pred_val = model.predict(X_val)\n",
    "\n",
    "        train_auc = roc_auc_score(y_true = y_train, y_score = pred_train)\n",
    "        val_auc = roc_auc_score(y_true = y_val, y_score = pred_val)\n",
    "\n",
    "        \n",
    "        res = pd.DataFrame([n, epochs, batch_size, stopped_epoch, train_auc, val_auc])\n",
    "        df = pd.concat([df, res], axis=1)\n",
    "\n",
    "        df.to_csv(os.path.join(path_results,'model_results_augmentation.csv')) \n",
    "                \n",
    "        model.save(os.path.join(path_results, 'model_augmentation_%d.h5' % n))\n",
    "            \n",
    "        if(previous_val_auc < val_auc):\n",
    "            save_dir = os.path.join(\n",
    "                    os.getcwd(),\n",
    "                    '../model_results_y')\n",
    "            if not os.path.isdir(save_dir):\n",
    "                    os.makedirs(save_dir)\n",
    "                model.save(os.path.join(path_results, 'model_no_augmentation_%d.h5' % n))\n",
    "                \n",
    "            previous_val_auc = val_auc\n",
    "                \n",
    "\n",
    "                           \n",
    "df.index = ['size_ae', 'epochs', 'batch_size','early_stopping', 'train_auc', 'val_auc']\n",
    "df.to_csv(os.path.join(path_results,'model_results_augmentation.csv'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train)\n",
    "pred_val = model.predict(X_val)\n",
    "pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_train = roc_auc_score(y_true = y_train, y_score = pred_train)\n",
    "auc_val = roc_auc_score(y_true = y_val, y_score = pred_val)\n",
    "auc_test = roc_auc_score(y_true = y_test, y_score = pred_test)\n",
    "print('AUC train = %s - AUC val = %s - AUC test = %s' % (str(auc_train), str(auc_val), str(auc_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels_train = (pred_train >= 0.5).astype(int)\n",
    "y_labels_val = (pred_val >= 0.5).astype(int)\n",
    "y_labels_test = (pred_test >= 0.5).astype(int)\n",
    "cm_train = confusion_matrix(y_pred = y_labels_train, y_true = y_train)\n",
    "cm_val = confusion_matrix(y_pred = y_labels_val, y_true = y_val)\n",
    "cm_test = confusion_matrix(y_pred = y_labels_test, y_true = y_test)\n",
    "print(cm_train)\n",
    "print(cm_val)\n",
    "print(cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fpr_train, tpr_train, threshold_train = roc_curve(y_train, pred_train)\n",
    "roc_auc_train = roc_auc_score(y_true = y_train, y_score = pred_train)\n",
    "fpr_val, tpr_val, threshold_val = roc_curve(y_val, pred_val)\n",
    "roc_auc_val = roc_auc_score(y_true = y_val, y_score = pred_val)\n",
    "fpr_test, tpr_test, threshold_test = roc_curve(y_test, pred_test)\n",
    "roc_auc_test = roc_auc_score(y_true = y_test, y_score = pred_test)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr_train, tpr_train, 'r', label = 'AUC = %0.2f' % roc_auc_test)\n",
    "plt.plot(fpr_val, tpr_val, 'g', label = 'AUC = %0.2f' % roc_auc_val)\n",
    "plt.plot(fpr_test, tpr_test, 'b', label = 'AUC = %0.2f' % roc_auc_test)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.xlim([-0.01, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr_train, tpr_train, 'b', label = 'AUC = %0.2f' % roc_auc_train)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([-0.01, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(os.path.join('../predictions_y', 'X_train.csv'))\n",
    "np.savetxt(os.path.join('../predictions_y', 'predictions_train.csv'), pred_train, delimiter=\";\")\n",
    "np.savetxt(os.path.join('../predictions_y', 'y_train.csv'), y_train, delimiter=\";\")\n",
    "X_val.to_csv(os.path.join('../predictions_y', 'X_val.csv'))\n",
    "np.savetxt(os.path.join('../predictions_y', 'predictions_val.csv'), pred_val, delimiter=\";\")\n",
    "np.savetxt(os.path.join('../predictions_y', 'y_val.csv'), y_val, delimiter=\";\")\n",
    "X_test.to_csv(os.path.join('../predictions_y', 'X_test.csv'))\n",
    "np.savetxt(os.path.join('../predictions_y', 'predictions_test.csv'),pred_test, delimiter=\";\")\n",
    "np.savetxt(os.path.join('../predictions_y', 'y_test.csv'), y_test, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_CI_train = bootstrap_auc(y_train, pred_train, bootstraps = 100, fold_size = 1000,)\n",
    "AUC_CI_val = bootstrap_auc(y_val, pred_val, bootstraps = 100, fold_size = 1000)\n",
    "AUC_CI_test = bootstrap_auc(y_test, pred_test, bootstraps = 100, fold_size = 1000,)\n",
    "AUC_CI = print_confidence_intervals(AUC_CI_train,)\n",
    "AUC_CI = AUC_CI.append(print_confidence_intervals(AUC_CI_val), ignore_index=True)\n",
    "AUC_CI = AUC_CI.append(print_confidence_intervals(AUC_CI_test), ignore_index=True)\n",
    "AUC_CI.index = ['Train', 'Val', 'Test'];\n",
    "AUC_CI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_train, tpr_train, threshold_train = roc_curve(y_train, pred_train)\n",
    "roc_auc_train = roc_auc_score(y_true = y_train, y_score = pred_train)\n",
    "fpr_val, tpr_val, threshold_val = roc_curve(y_val, pred_val)\n",
    "roc_auc_val = roc_auc_score(y_true = y_val, y_score = pred_val)\n",
    "fpr_test, tpr_test, threshold_test = roc_curve(y_test, pred_test)\n",
    "roc_auc_test = roc_auc_score(y_true = y_test, y_score = pred_test)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr_train, tpr_train, 'r', label = 'AUC = %0.2f' % roc_auc_train)\n",
    "plt.plot(fpr_val, tpr_val, 'g', label = 'AUC = %0.2f' % roc_auc_val)\n",
    "plt.plot(fpr_test, tpr_test, 'b', label = 'AUC = %0.2f' % roc_auc_test)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap / Gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.where(y_test == 1)[0]:\n",
    "    print('index ' + str(i));\n",
    "    heat_map, superimposed_img = show_heatmap(model, X_test[i], y_test[i], pred_test[i])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Keras_AE_convolutional.ipynb",
   "provenance": [
    {
     "file_id": "1cZ-eYScyMuVpqFzhP6KFWSACSTLcs9Gc",
     "timestamp": 1552989318329
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
